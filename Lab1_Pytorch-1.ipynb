{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <center>Lab 1: Introduction to Pytorch</center>\n",
        "\n",
        "We introduce the syntax of the [PyTorch](https://pytorch.org/) library for deep learning and optimization. Libraries such as PyTorch enable us to run massive computations on the GPU, which is essential for building large-scale neural models. A PyTorch project usually consists of the following components. While these are not very different from the typical components of a machine learning pipeline, they must be implemented in a specific way. \n",
        "\n",
        "- **Tensors**: The PyTorch `Tensor` can be thought of as analogous to the `numpy.ndarray`. We often will have tensors that are higher than 2D.\n",
        "- **Device**: The *device* refers to either CPU or GPU, depending on where models/tensors are stored, and computation happens.\n",
        "- **Dataset / Dataloader**: We instantiate (and sometimes implement) a `Dataset` class which specifies how to index the training, validation, and test data. This is then used to create a `DataLoader` class which specifies a scheme for loading batches of data, used both in training (e.g. stochastic gradient descent) and validation (e.g. batch-level accuracy). \n",
        "- **Architecture / Forward Pass**: We use a `Module` class to specify an *architecture* for our neural network ( layers, activations, etc.). The term *forward pass* refers to the sequence of computations that the network runs on an input given its parameters. Crucially, you implement the forward pass in the `forward` method, and the software uses automatic differentiation algorithms to compute the gradient without hand-coding it.\n",
        "- **Loss**: This is the final tensor which results from pushing the output of the network and the true labels through a function, usually called `criterion`. After this step, we call `backward` on the loss tensor in order to compute the gradient via the backpropagation algorithm (this is called the *backward pass*).\n",
        "- **Optimizer**: The `Optimizer` abstraction allows one to compute steps of iterative algorithms such as stochastic gradient descent. We call `step` on this object after the backward pass, which automatically runs the update step.\n",
        "\n",
        "<center> Adapted from teaching material created by Alec Greaves-Tunnell and Ronak Mehta, https://pytorch.org/  and https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html </center>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1 Pytorch Element: Tensors\n",
        "`Tensors` are specialized data structures and the building blocks of the Pytorch package, . The `Tensor` can be thought of as analogous to the `numpy.ndarray`. We often will have tensors that are higher than 2D and can be run on GPUs.\n",
        "Now, we will show the some examples using tensors instead of numpy arrays. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Creating Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ],
      "source": [
        "# Create data manually\n",
        "data = [[1, 2], [3, 4]]\n",
        "\n",
        "# Using numpy array\n",
        "data_np = np.array(data)\n",
        "print(data_np)\n",
        "\n",
        "# Using tensors\n",
        "x_data = torch.tensor(data)\n",
        "print(x_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ],
      "source": [
        "# Create data from numpy array\n",
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "print(x_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.9916, 0.9160, 0.4153],\n",
            "        [0.1395, 0.8509, 0.8796]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[0, 0],\n",
            "        [0, 0]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[1., 1.],\n",
            "        [1., 1.]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.8083, 0.6783],\n",
            "        [0.2024, 0.4519]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create tensor of all 1's or 0's or random\n",
        "shape = (2, 3)\n",
        "\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
        "\n",
        "ones_tensor = torch.ones(shape)\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "\n",
        "rand_tensor = torch.rand(shape)\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "\n",
        "# Create tensor using the shape of another tenosr\n",
        "x_ones = torch.zeros_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.ones_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensors have attributes similar to numpy arrays such as shape, datatype, and which device they are stored on. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.rand(3, 4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")\n",
        "\n",
        "# You can change the dtype or device by uinsg tensor.dtype() and tensor.device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Tensor Operations\n",
        "Almost all operations can be done using tensors and follows a very similar API as NumPy. We will cover just a few operations but a full list can be found [here](https://pytorch.org/docs/stable/torch.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full Tensor:  tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "Partial Tensor:  tensor([1., 1., 1., 1.])\n",
            "Edited Tensor: tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "# Indexing and Slicing\n",
        "tensor = torch.ones(4, 4)\n",
        "print(\"Full Tensor: \", tensor)\n",
        "print(\"Partial Tensor: \", tensor[:,3])\n",
        "\n",
        "# Replace column 2 with 0's\n",
        "tensor[:,1] = 0\n",
        "print(\"Edited Tensor:\", tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Join on Dim 0: tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "Join on Dim 1: tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "# Joining tensors: this can be done on a specific demension\n",
        "join_tensor_dim0 = torch.cat([tensor, tensor, tensor], dim=0)\n",
        "print(\"Join on Dim 0:\", join_tensor_dim0)\n",
        "\n",
        "join_tensor_dim1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(\"Join on Dim 1:\",join_tensor_dim1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can multiply tensor either element-wise or using matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor.mul(tensor) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor * tensor \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "# This computes the element-wise product\n",
        "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
        "\n",
        "# Alternative syntax:\n",
        "print(f\"tensor * tensor \\n {tensor * tensor}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor.matmul(tensor.T) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            "\n",
            "tensor @ tensor.T \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n"
          ]
        }
      ],
      "source": [
        "# This computes the matrix multplication between two tensors\n",
        "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
        "\n",
        "# Alternative syntax:\n",
        "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are called \"in-place\" operations and are denoted with the suffix `_`. They will change the tensor they are added to. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Tensor: tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "Add 5:  tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n",
            "Transpose:  tensor([[6., 6., 6., 6.],\n",
            "        [5., 5., 5., 5.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Original Tensor:\", tensor, \"\\n\")\n",
        "print(\"Add 5: \", tensor.add_(5))\n",
        "print(\"Transpose: \", tensor.t_())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Datasets and Dataloaders\n",
        "\n",
        "As mentioned, data needs to be in a specific form to use some of the machine learning functionality of Pytorch. To illustrate this, we will use a simple simulated dataset $(x_1, y_1), ..., (x_n, y_n)$ with each $y_i \\sim \\text{Bernoulli}\\left(\\frac{1}{2}\\right)$, and given $y$,\n",
        "$$\n",
        "x \\sim \\begin{cases}\n",
        "\\mathcal{N}(\\mu 1_d, \\sigma_1 I_d) &\\text{ if } y = 1\\\\\n",
        "\\mathcal{N}(-\\mu 1_d, \\sigma_0 I_d) &\\text{ if } y = 0\n",
        "\\end{cases}. \n",
        "$$\n",
        "To implement a `Dataset` class,  you must implement a `__len__` method (which returns the number of examples) and a `__getitem__` method, which returns the example at a particular index. Functions such as `TensorDataset` can make it so that you do not have to implement the class yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SimulatedDataset(Dataset):\n",
        "    def __init__(self, n, d, mean_scale=0.62, cov_scales=[1.0, 0.5]):\n",
        "        self.n = n\n",
        "        self.labels = torch.bernoulli(0.5 * torch.ones(n)).long()\n",
        "        distributions = [\n",
        "            torch.distributions.MultivariateNormal(\n",
        "                -mean_scale * torch.ones(d),\n",
        "                covariance_matrix=cov_scales[0] * torch.eye(d),\n",
        "            ),\n",
        "            torch.distributions.MultivariateNormal(\n",
        "                mean_scale * torch.ones(d),\n",
        "                covariance_matrix=cov_scales[1] * torch.eye(d),\n",
        "            ),\n",
        "        ]\n",
        "        self.examples = []\n",
        "        for i in range(n):\n",
        "            self.examples.append(distributions[int(self.labels[i])].sample())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.examples[index], self.labels[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([-0.3561,  0.2879,  0.1981,  0.1062, -0.0160,  0.3194,  0.4970, -0.1615,\n",
            "         0.3289,  0.2793]), tensor(1))\n"
          ]
        }
      ],
      "source": [
        "n = 100\n",
        "d = 10\n",
        "batch_size = 32\n",
        "val_size = 0.4\n",
        "\n",
        "dataset = SimulatedDataset(n, d)\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we have a `Dataset`, we can create a `DataLoader`, which is an iterable that cycles through batches of data. Some common schemes for sampling batches are the `RandomSampler` and `SequentialSampler`. Each batch will return a tuple based on the implementation of `__gititem__`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Batches\n",
            "Batch 0:\n",
            "\t X shape: torch.Size([32, 10])\n",
            "\t y shape: torch.Size([32])\n",
            "Batch 1:\n",
            "\t X shape: torch.Size([28, 10])\n",
            "\t y shape: torch.Size([28])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "\n",
        "val_len = int(len(dataset) * val_size)\n",
        "train_len = len(dataset) - val_len\n",
        "\n",
        "training_data, val_data = random_split(dataset, [train_len, val_len])\n",
        "\n",
        "train_dataloader = DataLoader(training_data, sampler=RandomSampler(training_data), batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n",
        "\n",
        "print(\"Train Batches\")\n",
        "for i, batch in enumerate(train_dataloader):\n",
        "    X_batch, y_batch = batch\n",
        "    \n",
        "    print(\"Batch %d:\" % i)\n",
        "    print(\"\\t X shape:\", X_batch.shape)\n",
        "    print(\"\\t y shape:\", y_batch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3: Model Architecture, Loss, and Optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Bag-of-Words Language Binary Classifier (Logistic Regression)\n",
        "\n",
        "Our model will map a sparse BoW representation to log probabilities over\n",
        "labels. We will use this to fit a binary (logistic) regression model. \n",
        "\n",
        "#### BoW\n",
        "We assign each word in the vocab an index. For example, say our\n",
        "entire vocab is two words \"hello\" and \"world\", with indices 0 and 1\n",
        "respectively. The BoW vector for the sentence \"hello hello hello hello\"\n",
        "is\n",
        "\n",
        "$[ 4, 0 ]$\n",
        "\n",
        "For \"hello world world hello\", it is\n",
        "\n",
        "$[ 2, 2 ]$\n",
        "\n",
        "etc. In general, it is\n",
        "\n",
        "$[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world})] $\n",
        "\n",
        "#### Example Data\n",
        "\n",
        "In this example we will have a list of sentences in either Spanish or English\n",
        "that are labeled with the correct language. We denote a BOW vector for each sentence as $x$. \n",
        "The output of our network is:\n",
        "\n",
        "$\\log \\text{Softmax}(Ax + b)$\n",
        "\n",
        "That is, we pass the input through an linear system (affine map) and then do log\n",
        "softmax.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n",
            "Vocab Size:  26\n"
          ]
        }
      ],
      "source": [
        "# The Data\n",
        "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
        "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
        "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
        "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
        "\n",
        "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
        "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
        "\n",
        "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}\n",
        "\n",
        "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
        "# index into the Bag of words vector\n",
        "word_to_ix = {}\n",
        "for sent, _ in data + test_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "print(\"Vocab Size: \", VOCAB_SIZE)\n",
        "NUM_LABELS = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Input: ['me', 'gusta', 'comer', 'en', 'la', 'cafeteria']\n",
            "tensor([[1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "torch.Size([1, 26])\n",
            "\n",
            "Sample Output: SPANISH\n",
            "tensor([0])\n",
            "torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "# Transform sentence into BoW\n",
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    for word in sentence:\n",
        "        vec[word_to_ix[word]] += 1\n",
        "    return vec.view(1, -1)\n",
        "\n",
        "\n",
        "def make_target(label, label_to_ix):\n",
        "    return torch.LongTensor([label_to_ix[label]])\n",
        "\n",
        "# Example\n",
        "print(\"Sample Input:\", data[0][0])\n",
        "print(make_bow_vector(data[0][0], word_to_ix))\n",
        "print(make_bow_vector(data[0][0], word_to_ix).size())\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"Sample Output:\", data[0][1])\n",
        "print(make_target(data[0][1], label_to_ix))\n",
        "print(make_target(data[0][1], label_to_ix).size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the data we can create the logisitic model. In PyTorch we generally use\n",
        "torch.nn.Module to build neural netowrks. This parent class contains many different classes\n",
        "to help build complex neural networks (linear, Conv1d, Transformer, etc.). We will be using \n",
        "nn.Linear but documentation on the other choices can be found [here](https://pytorch.org/docs/stable/nn.html#linear-layers).\n",
        "\n",
        "Torch.nn.Module always consist of a `_init_` and `forward` function. Also, for each layer of your neural network you will need to provide the input and output sizes. More information on torch.nn.Module can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.1655,  0.1671,  0.1390,  0.1662, -0.1932, -0.0908,  0.0526, -0.0791,\n",
            "          0.1503,  0.1018, -0.1878,  0.0487,  0.0638,  0.0197, -0.0379,  0.1740,\n",
            "          0.0890, -0.1647, -0.1136,  0.1922,  0.0229,  0.0801, -0.0531, -0.1287,\n",
            "          0.1897, -0.1934],\n",
            "        [ 0.0682, -0.0177,  0.1369, -0.1165, -0.1008,  0.1811, -0.1784, -0.0171,\n",
            "          0.1763, -0.0209, -0.1126, -0.1333,  0.1108,  0.1652, -0.0051, -0.0956,\n",
            "          0.1906,  0.1741, -0.0285,  0.1515, -0.1415,  0.0501,  0.1052,  0.1945,\n",
            "          0.0713, -0.1223]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0286, -0.0707], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BinaryClassifier(nn.Module):  # using nn.Module class!\n",
        "\n",
        "    def __init__(self, num_labels, vocab_size):\n",
        "        # Calls the init function of nn.Module.  \n",
        "        # It allows you to call the methods from inside the nn.Module parent class.\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "\n",
        "        # Define the parameters that you will need.  In this case, we need A and b,\n",
        "        # the parameters of the linear mapping.\n",
        "        # Torch defines nn.Linear(), which provides the affine map.\n",
        "        # Why is the input dimension is vocab_size and the output is num_labels?\n",
        "        self.linear = nn.Linear(vocab_size, num_labels)\n",
        "\n",
        "    def forward(self, bow_vec):\n",
        "        # Pass the input through the linear layer,\n",
        "        # then pass that through log_softmax (since we are doing logistic regression).\n",
        "        # Many non-linearities and other functions are in torch.nn.functional\n",
        "        return nn.functional.log_softmax(self.linear(bow_vec), dim=1)\n",
        "\n",
        "model = BinaryClassifier(NUM_LABELS, VOCAB_SIZE)\n",
        "\n",
        "# The model knows its parameters since we used nn.Linear().  The first output below is A, the second is b.\n",
        "# Whenever you assign a component to a class variable in the __init__ function of a module, which was done with the line\n",
        "# self.linear = nn.Linear(...) then through your module (in this case, BinaryClassifier) will store knowledge of the nn.Linear's parameters.\n",
        "for param in model.parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample: (['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH')\n",
            "Output: tensor([[-0.5782, -0.8230]])\n"
          ]
        }
      ],
      "source": [
        "# To run the model, pass in a BoW vector\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    sample = data[0]\n",
        "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
        "    log_probs = model(bow_vector)\n",
        "    print(\"Sample:\", data[0])\n",
        "    print(\"Output:\", log_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So lets train! To do this, we pass instances through to get log\n",
        "probabilities, compute a loss function, compute the gradient of the loss\n",
        "function, and then update the parameters with a gradient step. \n",
        "Loss\n",
        "functions are provided by Torch in the nn package. We will use `nn.NLLLoss()` which is the\n",
        "negative log likelihood loss. Other loss functions can be found [here](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
        "\n",
        "It also defines optimization\n",
        "functions in torch.optim. Here, we will just use SGD. Other optimization functions can be found [here](https://pytorch.org/docs/stable/optim.html).\n",
        "\n",
        "Note that the *input* to NLLLoss is a vector of log probabilities, and a\n",
        "target label. It doesn't compute the log probabilities for us. This is\n",
        "why the last layer of our network is log softmax. The loss function\n",
        "`nn.CrossEntropyLoss()` is the same as `NLLLoss()`, except it does the log\n",
        "softmax for you.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Test Output\n",
            "tensor([[-0.7215, -0.6656]])\n",
            "tensor([[-0.7263, -0.6610]])\n",
            "Probabilities of 'Creo': tensor([-0.1878, -0.1126], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Run on test data before we train, just to see a before-and-after\n",
        "print(\"Original Test Output\")\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        log_probs = model(bow_vec)\n",
        "        print(log_probs)\n",
        "\n",
        "# Print the matrix column corresponding to \"creo\"\n",
        "print(\"Probabilities of 'Creo':\", next(model.parameters())[:, word_to_ix[\"creo\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we train the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Test Values:  [(['Yo', 'creo', 'que', 'si'], 'SPANISH'), (['it', 'is', 'lost', 'on', 'me'], 'ENGLISH')]\n",
            "tensor([[-0.1734, -1.8377]])\n",
            "tensor([[-2.8461, -0.0598]])\n",
            "Probabilities of 'Creo': tensor([ 0.2199, -0.5202], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Usually you want to pass over the training data several times.\n",
        "# 100 is much bigger than on a real data set, but real datasets have more than\n",
        "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
        "for epoch in range(100):\n",
        "    for instance, label in data:\n",
        "        # Step 1. PyTorch accumulates gradients.\n",
        "        # We need to clear them out before each instance - we will discuss this more later\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
        "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
        "        # we wrap the integer 0. The loss function then knows that the 0th\n",
        "        # element of the log probabilities is the log probability\n",
        "        # corresponding to SPANISH\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        target = make_target(label, label_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        log_probs = model(bow_vec)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss = loss_function(log_probs, target)\n",
        "        loss.backward() # computes the gradient of the loss\n",
        "        optimizer.step() # updates parameters using the gradient of the loss\n",
        "print(\"True Test Values: \", test_data)\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        log_probs = model(bow_vec)\n",
        "        print(log_probs)\n",
        "\n",
        "# Index corresponding to Spanish goes up, English goes down!\n",
        "print(\"Probabilities of 'Creo':\", next(model.parameters())[:, word_to_ix[\"creo\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We got the right answer! You can see that the log probability for\n",
        "Spanish is much higher in the first example, and the log probability for\n",
        "English is much higher in the second for the test data, as it should be. \n",
        "Also, the we see the specific log probability for the word \"Creo\" for Spanish is \n",
        "much larger than English.\n",
        "\n",
        "Now you see how to make a PyTorch component, pass some data through it\n",
        "and do gradient updates. We are ready to dig deeper into what PyTorch\n",
        "has to offer.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data598",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "86858d5f84902b050d54a6bd9c0fb878f86a1dbd7a1ae5d42a558ee921f88707"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
