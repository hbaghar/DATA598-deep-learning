{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Lecture 6: Embeddings and ML Experiments </h1> </center>\n",
    "<center> Jillian Fisher, Zaid Harchaoui </center>\n",
    "    <center> Data 598 (Winter 2023), University of Washington </center>\n",
    "\n",
    "We will discuss two topics this lecture:\n",
    "- Embeddings for natural language\n",
    "- Model Selection with statistical tests\n",
    "\n",
    "This notebook is inspired by the [D2L book](https://d2l.ai/),  and adapted from lecture material created by Krishna Pillutla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Embeddings for Natural Language\n",
    "\n",
    "The field of **natural language processing (NLP)** is concerned with the interaction between computers and natural (human) language. This involves \"understanding\" the contents of documents, including the contextual nuances of the language within them. \n",
    "\n",
    "**Embeddings**:\n",
    "The use of machine learning for NLP, both in the classical settings as well as the modern deep learning era, have relied on *embedding* words in vector spaces.\n",
    "Words are made of characters, which are combinatorial in nature with no \"neighborhood\" structure which one expects of vectors in, say, a Euclidean space. \n",
    "The magic of embeddings is that they are able to capture some \"neighborhood\" structure in words, e.g., the embedding of synonyms are closer together than of words which have nothing in common. \n",
    "\n",
    "![](https://miro.medium.com/max/2400/1*OEmWDt4eztOcm5pr2QbxfA.png)\n",
    "Image credits: https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8\n",
    "\n",
    "**Note**: Sometimes, we will work at the level of subword units, rather than words. Mathematically, the same treatment holds irrespective of how we *tokenize* the text. We will refer to these units as *tokens*.\n",
    "\n",
    "\n",
    "**Types of embeddings**:\n",
    "\n",
    "- Global (context-free) embeddings: TF-IDF, word2vec, GloVe\n",
    "- Contextual embeddings: ELMo, BERT, ...\n",
    "\n",
    "![](http://ai.stanford.edu/blog/assets/img/posts/2020-03-24-contextual/contextual_mouse_transparent_2.png)\n",
    "Image credit: http://ai.stanford.edu/blog/contextual/\n",
    "\n",
    "\n",
    "**The history of word embeddings**:\n",
    "The research started with global (context-free) embeddings with \n",
    "later research producing contextual embeddings using deep learning.\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\text{TF-IDF}   & \\text{word2vec}   &  \\text{GloVe}   &   \\text{ELMo}     &       \\text{BERT}  \\\\\n",
    "2003       &2013       &  2014    &   2017     &       2018 \n",
    "\\end{matrix} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Word Embeddings:\n",
    "We will walk through the different methods of constructing word embeddings and how they differ. \n",
    "\n",
    "### Context-free Embeddings\n",
    "Context-free embeddings involves representing a word irrespective of the meaning of that word in a particular sentence (pre-determined). \n",
    "Therefore, context-free embedding methods always return a single word embedding for each word in the vocabulary. \n",
    "\n",
    "We have already seen an example of context-free embedding, Bag of Words. Here we will describe a few other methods. \n",
    "\n",
    "### **1. Term frequency-inverse document frequency (TF-IDF)**\n",
    "TF-IDF is a method which combines two metrics to provide a score for each word (in each document). \n",
    "\n",
    "It uses the following:\n",
    "\n",
    "1. *Term frequency (TF)*: the frequency of words in a particular document\n",
    "\n",
    "$TF_{i,j} = \\frac{\\text{Term }i \\text{ frequency in document }j}{\\text{Total num. of terms in document }j}$\n",
    "\n",
    "2. *Inverse document frequency (IDF)*: the rarity of words in the text\n",
    "\n",
    "$IDF_{i} = \\log \\frac{\\text{Total documents}}{\\text{Num. of documents containing term }i}$ \n",
    "\n",
    "$\\text{TF-IDF Score} = TF * IDF$\n",
    "\n",
    "Uses: basic nlp analysis, information retrieval, stop words removal \n",
    "\n",
    "Challenges: d not capture semantic meaning, as vocabulary size increases so does the vector representation (sparse vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           TF-IDF Doc1  TF-IDF Doc2\n",
      "at            0.334712     0.278675\n",
      "bank          0.334712     0.278675\n",
      "collected     0.470426     0.000000\n",
      "me            0.000000     0.391668\n",
      "meet          0.000000     0.391668\n",
      "my            0.470426     0.000000\n",
      "paycheck      0.470426     0.000000\n",
      "please        0.000000     0.391668\n",
      "river         0.000000     0.391668\n",
      "the           0.334712     0.278675\n",
      "tomorrow      0.000000     0.391668\n"
     ]
    }
   ],
   "source": [
    "# Use Scikit-learn for TF-IDF Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd \n",
    "\n",
    "sentence1 = \"I collected my paycheck at the bank\" # \"document 1\"\n",
    "sentence2 = \"Please meet me tomorrow at the river bank\" # \"document 2\"\n",
    "\n",
    "corpus = [sentence1, sentence2]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfIdf = vectorizer.fit_transform(corpus)\n",
    "# vectorizer.get_feature_names_out()\n",
    "\n",
    "df = pd.DataFrame(tfIdf.T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"TF-IDF Doc1\", \"TF-IDF Doc2\"])\n",
    "# df = df.sort_values('TF-IDF', ascending=False)\n",
    "print (df.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Word2Vec**\n",
    "This method was developed by Google in 2013 and was the dominate method until the release of Transformer (around 2018). This method attempts to incorporate semantic meaning by considering the order of the words (in the past and in the future). It actually uses a shallow neural network (input, output, and projection layer) to create the embeddings. It centers on the hypothesis that neighboring words have semantic similarities. But how do we express words being \"semantically similar\"?\n",
    "\n",
    "*Cosine Similarity*\n",
    "\n",
    "In NLP we generally use the \"cosine similarity\" metric to indicate semantically close. This finds the cosine angle between two vectors (equation below). So if the cosine similarity is 1 than the two words are exactly semantically similar and if it is 0 than they are not semantically similar at all. \n",
    "\n",
    "$\\text{cosine similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^n A_i B_i}{\\sqrt{\\sum_{i=1}^n A_i^2}\\sqrt{\\sum_{i=1}^n B_i^2}}$\n",
    "\n",
    "**Continuous Bag of Words (CBOW)**\n",
    "Word2Vec uses CBOW to code the input to it's shallow network. \n",
    "\n",
    "<img src=\"https://d2mk45aasx86xg.cloudfront.net/Context_and_current_word_in_CBOW_d75636ea68.webp\" width=\"400\">\n",
    "Image credit: https://www.turing.com/kb/guide-on-word-embeddings-in-nlp\n",
    "\n",
    "CBOW centers each word around a window (set by the user) and then uses one-hot encoding to create inputs to the shallow network.\n",
    "The shallow network than try to predict the current word using the surrounding words. \n",
    "\n",
    "<img src=\"https://d2mk45aasx86xg.cloudfront.net/Continuous_bag_of_word_embedding_in_NLP_7ea4656378.webp\" width=\"400\">\n",
    "Image credit: https://www.turing.com/kb/guide-on-word-embeddings-in-nlp\n",
    "\n",
    "Another method for Word2Vec is using skip-grams, see [here](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp) for more details. \n",
    "\n",
    "Uses: can use for semantic vectorization\n",
    "\n",
    "Challenges: can be computationally intensive and relies on a linear window for semantics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jillianfisher/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Word Vector for 'Bank':  100\n",
      "Cosine similarity between 'river' and 'bank':  -0.095753424\n",
      "Cosine similarity between 'bank' and 'bank':  1.0\n",
      "Top 3 most similar words to'bank':  [('at', 0.06797593086957932), ('please', 0.033640600740909576), ('tomorrow', 0.009394742548465729)]\n"
     ]
    }
   ],
   "source": [
    "# Install the nltk and gensim\n",
    "# Important: make sure pip is installed in your conda environment\n",
    "# Run \"pip install nltk\" and \"pip install gensim\" in your terminal\n",
    "# Python program to generate word vectors using Word2Vec\n",
    " \n",
    "# importing all necessary modules\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    " \n",
    "#  Same data as before\n",
    "sentence1 = \"I collected my paycheck at the bank\" # \"document 1\"\n",
    "sentence2 = \"Please meet me tomorrow at the river bank\" # \"document 2\"\n",
    "\n",
    "# sent_tokenizer automatically sperates based on punctuation\n",
    "corpus = sentence1 +\". \" +sentence2 +\". \"\n",
    "\n",
    "data = []\n",
    " \n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(corpus):\n",
    "    temp = []\n",
    "     \n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    " \n",
    "    data.append(temp)\n",
    " \n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,\n",
    "                              vector_size = 100, window = 3)\n",
    " \n",
    "# Print results\n",
    "print(\"Length of Word Vector for 'Bank': \", len(model1.wv[\"bank\"]))\n",
    "print(\"Cosine similarity between 'river' \" +\n",
    "            \"and 'bank': \",\n",
    "model1.wv.similarity('paycheck', 'bank'))\n",
    "print(\"Cosine similarity between 'bank' \" +\n",
    "            \"and 'bank': \",\n",
    "model1.wv.similarity('bank', 'bank'))\n",
    "\n",
    "print(\"Top 3 most similar words to'bank': \", model1.wv.most_similar('bank', topn=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Embeddings\n",
    "### **1. Bidirectional encoder representations for transformers (BERT)**\n",
    "Similar to Word2Vec, BERT uses a model to create the word vectors. However, instead of simple model, the BERT model is a deep neural network with 12 layers (a transformer). We will learn about transformers in-depthly next week, so for today we will just play with the embeddings from BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Playing with embeddings**:\n",
    "We will play with BERT embeddings, a form of contextual embeddings, using the `transformers` library.\n",
    "\n",
    "BERT and its follow-ups such as RoBERTa provided contextual embeddings that depend on both the left context and right context of a token. \n",
    "On the other hand, GPT-2 and GPT-3 produce representations that only depend on the left context. This comes from how these language models are trained. We will talk about the different pretraining strategies in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install the transformers library\n",
    "# Important: make sure pip is installed in your conda environment\n",
    "# Run \"pip install transformers\" in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "# Download the pre-trained model + tokenizer (a total of 440 MB)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name) # to tokenize the text\n",
    "model = BertModel.from_pretrained(model_name)  # PyTorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: tensor([[ 101, 1045, 5067, 2026, 3477, 5403, 3600, 2012, 1996, 2924,  102]])\n",
      "Sentence 2: tensor([[ 101, 3113, 2033, 4826, 2012, 1996, 2314, 2924,  102]])\n",
      "Sentence 1 Length: torch.Size([1, 11])\n",
      "Sentence 2 Length: torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "# Consider these two sentences\n",
    "\n",
    "sentence1 = \"I collected my paycheck at the bank\"\n",
    "sentence2 = \"Meet me tomorrow at the river bank\"\n",
    "\n",
    "# Let us tokenize them \n",
    "tokens_for_sentence1 = tokenizer.encode(sentence1, return_tensors='pt')\n",
    "tokens_for_sentence2 = tokenizer.encode(sentence2, return_tensors='pt')\n",
    "\n",
    "print('Sentence 1:', tokens_for_sentence1)\n",
    "print('Sentence 2:', tokens_for_sentence2)\n",
    "\n",
    "print('Sentence 1 Length:', tokens_for_sentence1.shape)\n",
    "print('Sentence 2 Length:', tokens_for_sentence2.shape)\n",
    "# the leading 1 is the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'collected', 'my', 'pay', '##che', '##ck', 'at', 'the', 'bank', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Look at the word peices the tokens correpond to: \n",
    "print(tokenizer.convert_ids_to_tokens(tokens_for_sentence1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token \"2924\" corresponds to the word \"bank\".\n",
    "Observe now that the contextual embedding of the word \"bank\" for each case is different. \n",
    "This would not have been the case for a global embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 768])\n",
      "torch.Size([1, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs1 = model(tokens_for_sentence1,\n",
    "                return_dict=True)\n",
    "\n",
    "# Extract contextual embedding for each token\n",
    "embeddings_for_sentence1 = outputs1.last_hidden_state\n",
    "print(embeddings_for_sentence1.shape) # [batch_size, num_tokens, dimension]\n",
    "\n",
    "outputs2 = model(tokens_for_sentence2,\n",
    "                return_dict=True)\n",
    "\n",
    "# Extract contextual embedding for each token\n",
    "embeddings_for_sentence2 = outputs2.last_hidden_state\n",
    "print(embeddings_for_sentence2.shape) # [batch_size, num_tokens, dimension]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 distance between the embeddings: 11.550871849060059\n"
     ]
    }
   ],
   "source": [
    "# The last token 102 is a special [SEP]. The second-to-last token 2924 corresponds to \"bank\".\n",
    "embedding_for_bank_1 = embeddings_for_sentence1[0, -2, :]\n",
    "embedding_for_bank_2 = embeddings_for_sentence2[0, -2, :]\n",
    "print('L2 distance between the embeddings:', \n",
    "      torch.norm(embedding_for_bank_1-embedding_for_bank_2).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the L2 distance here as an example to show that the embeddings of \"bank\" in each sentence are distinct.\n",
    "A more commonly used measure of similarity/distance between embeddings is the cosine similarity/distance.\n",
    "\n",
    "The cosine similarity between two vectors $v_1$ and $v_2$ is defined as:\n",
    "$$\n",
    "    S(v_1, v_2) = \\frac{v_1^\\top v_2}{ \\|v_1\\| \\, \\|v_2\\|} .\n",
    "$$\n",
    "\n",
    "**Exercise**: When $v_1$ and $v_2$ are unit vectors, show that $\\|v_1 - v_2\\|^2 = 2\\big(1 - S(v_1, v_2)\\big)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using Embeddings\n",
    "\n",
    "We will look at the standard NLP task of sentiment analysis. \n",
    "Given a piece of text, the goal is to classify it as \"positive\" or \"negative\" in sentiment.\n",
    "\n",
    "![](https://vitalflux.com/wp-content/uploads/2021/10/sentiment-analysis-machine-learning-techniques-640x395.png)\n",
    "Image credits: https://vitalflux.com/sentiment-analysis-machine-learning-techniques/\n",
    "\n",
    "Our procedure is as follows:\n",
    "- We will use a labeled dataset and cast this as a multiclass classification problem\n",
    "- We will use these BERT embeddings to construct obtain one vector per token. We will simply take the mean of this vector as the feature representation of the entire piece of text.\n",
    "- We will train a simple linear model to predict the output label from these features.\n",
    "\n",
    "\n",
    "Download the data from [here](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data?select=train.tsv.zip).\n",
    "\n",
    "We will use movie reviews from Rotten Tomatoes. The sentiment labels are:\n",
    "- 0 - negative\n",
    "- 1 - somewhat negative\n",
    "- 2 - neutral\n",
    "- 3 - somewhat positive\n",
    "- 4 - positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8529, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SentenceId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Phrase  Sentiment\n",
       "SentenceId                                                              \n",
       "1           A series of escapades demonstrating the adage ...          1\n",
       "2           This quiet , introspective and entertaining in...          4\n",
       "3           Even fans of Ismail Merchant 's work , I suspe...          1\n",
       "4           A positively thrilling combination of ethnogra...          3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filename = './data/train.tsv'\n",
    "# keep one example per sentence (original data labels each phrase)\n",
    "data = pd.read_csv(filename, sep='\\t').groupby('SentenceId').first()\n",
    "data = data.drop(columns=['PhraseId'])\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Even fans of Ismail Merchant 's work , I suspect , would have a hard time sitting through this one .\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.at[3, 'Phrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split and featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "data = data.sample(frac=1)  # shuffle\n",
    "train_data = data[:1000]\n",
    "test_data = data[5000:6000]\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def featurize(x): # x is pd.Series with text\n",
    "    features = []\n",
    "    for sen in tqdm(x):\n",
    "        sen = tokenizer.encode(sen, return_tensors='pt')\n",
    "        outputs = model(sen, return_dict=True)\n",
    "        embeddings = outputs.last_hidden_state.squeeze() # (len, dim)\n",
    "        mean_embedding = embeddings.mean(axis=0)\n",
    "        features.append(mean_embedding.numpy())\n",
    "    return np.stack(features)  # (n, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02a068a18374bce85d5460daa1ae1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe2c92067ff4d80b649ea6116dac360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Takes a few minutes to run\n",
    "x_train = featurize(train_data['Phrase'])\n",
    "y_train = train_data['Sentiment'].values\n",
    "\n",
    "x_test = featurize(test_data['Phrase'])\n",
    "y_test = test_data['Sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a simple logistic regression classifier to test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99, random_state=1).fit(x_train)  # keep 99% of the explained variance\n",
    "x_train = pca.transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.543\n",
      "Test accuracy: 0.426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, C=0.01).fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(x_train)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "\n",
    "print('Train accuracy:', (y_train_pred == y_train).mean())\n",
    "print('Test accuracy:', (y_test_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improving this model**: \n",
    "In this example, we simply averaged the embeddings of all the tokens\n",
    "from one example to get an embedding for the entire example. We then train a linear model. \n",
    "Here are two ways to improve this model: \n",
    "1. Instead of averaging the embeddings of each token, we train a recurrent neural network that maintains the sequential nature of the data. We will take a closer look into recurrent networks next week. \n",
    "2. The preferred approach today is to attach a prediction layer to the BERT model using the embedding of the [CLS] token (this is special token we added to the start of the sequence). We then finetune the entire BERT model on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Statistical Tests for Analysis for ML Experiments\n",
    "\n",
    "In some particular safety-critical applications, it might be necessary to make guarantees of the form \n",
    "\"*The misclassification error of my classifier is at most 12% on data from the same distribution as our training data*\". Think of self-driving cars, for instance. The 12% above is an arbitrary number.\n",
    "\n",
    "In these cases, we run hypothesis tests to formalize our claims.\n",
    "\n",
    "**Hypothesis Testing Review**: \n",
    "Suppose we want to show that herbal tea helps with migraines. \n",
    "In the spirit of \"proof by contradiction\", \n",
    "we assume the opposite to be true and say that herbal tea does not help with migraines. \n",
    "If the data looks too \"anomalous\" under this assumption, we arrive at a \n",
    "\"contradiction\", which means that the data is not consistent the \n",
    "claim that \"herbal tea does not help with migraines\" (the opposite of what we set out to show).\n",
    "\n",
    "\n",
    "We have two hypotheses, the null hypothesis (denoted $H_0$; the opposite of what we want to show) and the alternate hypothesis (denoted $H_1$ or $H_a$; what we want to show). \n",
    "From looking at the data, we take one of two steps:\n",
    "- reject the null hypothesis\n",
    "- \"fail\" to reject the null hypothesis\n",
    "\n",
    "\n",
    "**Illustration**: ([credit](https://study.com/cimages/multimages/16/ea0e233d-7bc3-4ba6-a79c-5d8281295985_t_tests.png)): \n",
    "\n",
    "Suppose we are given two distributions with means $\\mu_1$ and $\\mu_2$ respectively. \n",
    "We plot as test statistic (TS) the difference $\\hat\\mu_{1, n} - \\hat\\mu_{2, n}$ between the sample means. The bell-curve is centered at $0$.\n",
    "![](https://study.com/cimages/multimages/16/ea0e233d-7bc3-4ba6-a79c-5d8281295985_t_tests.png)\n",
    "\n",
    "\n",
    "Letting \"acc $(h)$ \" denote the classification accuracy of our classification algorithm $h$, We may write this test as the following:\n",
    "$$\n",
    "H_0: \\quad \\text{acc}(h) \\le a_0 \\\\\n",
    "H_1: \\quad \\text{acc}(h) > a_0 ,\n",
    "$$\n",
    "where $a_0$ is some pre-specified accuracy.\n",
    "\n",
    "The outcomes are:\n",
    "- Reject the null: If our data is convincing enough (i.e., the accuracy on our validation set is significantly larger than $a_0$), we reject the null with a certain level of confidence\n",
    "- Fail to reject the null: If the validation accuracy is close to or smaller than $a_0$, we say that we do not have strong enough evidence to reject the null (default) hypothesis. \n",
    "\n",
    "**The $t$-Test to Assess Classification**:\n",
    "\n",
    "Suppose we have $K$ training-validation pairs. For each one, we record the \n",
    "validation accuracies $A_1, \\cdots, A_K$. \n",
    "The empirical mean and variance are:\n",
    "$$\n",
    "    m = \\frac{1}{K} \\sum_{k=1}^K A_k\\,, \\quad S^2 = \\frac{1}{K-1}(A_k - m)^2 \\,.\n",
    "$$\n",
    "The test statistic is then\n",
    "$$\n",
    "    T_K := \\frac{\\sqrt{K}(m - a_0)}{S} .\n",
    "$$\n",
    "Under the assumption of independence of each of the training-validation set pairs, \n",
    "it turns out that $T_K$ is distributed according to the Student $t$ distribution with $K-1$ degrees of freedom. \n",
    "\n",
    "In this case, we reject the null with a level of significance $\\alpha$ if \n",
    "$$\n",
    "    T_K > t_{K-1, \\alpha},\n",
    "$$\n",
    "the $(1-\\alpha)$-quantile of of the $t_{K-1}$ distribution. \n",
    "\n",
    "That is, we reject the null if \n",
    "$$\n",
    "    m > a_0 + \\frac{S}{\\sqrt{K}} t_{K-1, \\alpha} \\,.\n",
    "$$\n",
    "Observe what happens as $K$ grows or $\\alpha$ becomes smaller. \n",
    "![](https://lh3.googleusercontent.com/proxy/Rk0TX6KUcZLaFgMU42Qr553ALEHXt1YRIoZRIZfaoTMp69H5UcESVWmj3C-qE1NgtSUyngFqUx-v_O9__tzq29yUeZ3OKcmwVbby2bJ5neKzkBBFGzJhQzR9U0rWxL3kEYYV7ieeZh8hvCfLffhyP2AghYESkJqOa7fg5qAj)\n",
    "\n",
    "The significance level $\\alpha$ is the type-I error: the probability of rejecting the null hypothesis when it is correct. \n",
    "The type-II error is the probability of failing to reject the null when the alternate is correct; this is related to the *power* of the test. \n",
    "\n",
    "**Illustration**: What is the null hypothesis here?\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-a25c9f17379bd7b94719a77686dfb519)\n",
    "Image source: https://effectsizefaq.com/2010/05/31/i-always-get-confused-about-type-i-and-ii-errors-can-you-show-me-something-to-help-me-remember-the-difference/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $t$-test in action\n",
    "Let us assess the accuracy of one of the ConvNets we saw in Week 2. We will construct $5$ different training-validation splits of the data. \n",
    "We will test for the following:\n",
    "\n",
    "$$\n",
    "H_0: \\text{accuracy} \\le 0.87 \\\\\n",
    "H_1: \\text{accuracy} > 0.87\n",
    "$$\n",
    "\n",
    "We will use a significance level of $\\alpha = 0.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torch.nn.functional import cross_entropy\n",
    "import time\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the FashionMNIST dataset and divide it into 5 train-val pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d6efd537734e3e90f2feae789f5bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d60094162b449a959d4a7347522f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65c889000c4405ea9798c4a921ac95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58026b2b5184facb385db404ad934bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "\n",
    "\n",
    "# shuffle the data\n",
    "idxs = np.random.permutation(X_train.shape[0])\n",
    "size = X_train.shape[0]//10\n",
    "\n",
    "Xs = [] \n",
    "ys = []\n",
    "for i in range(10): # 5 train-val pairs\n",
    "    subsample_idxs = idxs[i*size : (i+1)*size]\n",
    "    X = X_train[subsample_idxs]\n",
    "    y = y_train[subsample_idxs]\n",
    "    Xs.append(X)\n",
    "    ys.append(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the model and our helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_ensemble_1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.conv_ensemble_2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.fc = torch.nn.Linear(7*7*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        out = self.conv_ensemble_1(x)\n",
    "        out = self.conv_ensemble_2(out)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "# Some utility functions to compute the objective and the accuracy\n",
    "def compute_objective(model, X, y):\n",
    "    score = model(X)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return cross_entropy(input=score, target=y, reduction='mean') \n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(model, X, y):\n",
    "    score = model(X)\n",
    "    predictions = torch.argmax(score, axis=1)  # class with highest score is predicted\n",
    "    return (predictions == y).sum() * 1.0 / y.shape[0]\n",
    "\n",
    "def sgd_one_pass(model, X, y, learning_rate, verbose=False):\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "        idx = np.random.choice(X.shape[0])\n",
    "        # compute the objective. \n",
    "        # Note: This function requires X to be of shape (n,d). In this case, n=1 \n",
    "        objective = compute_objective(model, X[idx:idx+1], y[idx:idx+1]) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=model.parameters())\n",
    "        \n",
    "        # perform SGD update. IMPORTANT: Make the update inplace!\n",
    "        for (w, g) in zip(model.parameters(), gradients):\n",
    "            w.data -= learning_rate * g.data\n",
    "      \n",
    "    \n",
    "from tqdm.auto import trange # range + progress bar\n",
    "def sgd_n_passes(X_train, y_train, X_val, y_val, n_passes, learning_rate):\n",
    "    model = ConvNet()\n",
    "    for i in trange(n_passes):\n",
    "        sgd_one_pass(model, X_train, y_train, learning_rate)\n",
    "    return compute_accuracy(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37648ca01c89498393ab66e82aa40650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84507fc49c6e4e1fa44a25cdccfb4604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c05fc6526a64591b27451381c76c083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86d5c6e14064706b79e27f12400c1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eed277fe01842c09d0dc101d5eb9aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = []\n",
    "for i in range(5):\n",
    "    print(f'Starting run {i+1}')\n",
    "    X_train, y_train = Xs[2*i], ys[2*i]\n",
    "    X_val, y_val = Xs[2*i+1], ys[2*i+1]\n",
    "    acc = sgd_n_passes(X_train, y_train, X_val, y_val, n_passes=30, learning_rate=2.5e-3)\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.879     , 0.87666667, 0.8756667 , 0.8765    , 0.8721667 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print accuracies\n",
    "accuracies = np.asarray(accuracies)\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: 5.421106851044113\t threshold: 2.13184678133629\n",
      "Reject the null\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05  # significance level\n",
    "a_0 = 0.87 # accuracy level we are testing for\n",
    "K = accuracies.shape[0]\n",
    "m = np.mean(accuracies)\n",
    "s = np.std(accuracies, ddof=1)  # divide by K-1\n",
    "\n",
    "# Compute the test statistic\n",
    "T =  np.sqrt(K) * (m - a_0) / s\n",
    "threshold = scipy.stats.t(df=K-1).ppf(1-alpha)  # 1-alpha quantile of t_{K-1}\n",
    "\n",
    "print(f'Test statistic: {T}\\t threshold: {threshold}')\n",
    "\n",
    "if T > threshold:\n",
    "    print('Reject the null')\n",
    "else:\n",
    "    print('Fail to reject the null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can work through what we would have gotten if were to test for \n",
    "accuracy being at least 88%. \n",
    "\n",
    "**NOTE**: We must determine the hypotheses before running the task. We are not supposed to adaptively change the test depending on the results. This is only \"a simulation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: -3.614000865536312\t threshold: 2.13184678133629\n",
      "Fail to reject the null\n"
     ]
    }
   ],
   "source": [
    "a_0 = 0.88 \n",
    "# Compute the test statistic\n",
    "T =  T = np.sqrt(K) * (m - a_0) / s\n",
    "threshold = scipy.stats.t(df=K-1).ppf(1-alpha)  # 1-alpha quantile of t_{K-1}\n",
    "\n",
    "print(f'Test statistic: {T}\\t threshold: {threshold}')\n",
    "\n",
    "if T > threshold:\n",
    "    print('Reject the null')\n",
    "else:\n",
    "    print('Fail to reject the null')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "694a2605e6c4cb7e5ecd12cf55a62475b886d9de17fe0247424478eb05601482"
  },
  "kernelspec": {
   "display_name": "Python 3.8.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
