{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  <h1> Data 598 (Winter 2023): Midterm </h1> </center> \n",
    "    <center> University of Washington </center>\n",
    "    \n",
    "\n",
    "There are four parts in this midterm. \n",
    "1. Understanding automatic differentiation\n",
    "1. Programming a differentiable module\n",
    "1. Optimization a neural network\n",
    "1. Explaining concepts from convolutional neural nets\n",
    "\n",
    "**Rules**:\n",
    "- No collaborations allowed! You must work on your own.\n",
    "- You may refer to the reference books.\n",
    "- You may refer to the labs, demos, lab solutions and homework solutions. \n",
    "- However, you may not directly copy-paste code from any of the labs or demos. You should write your own code.\n",
    "- The only exception to the copy-paste rule is where explicitly specified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Understanding automatic differentiation\n",
    "\n",
    "Consider the function $f: \\mathbb{R} \\to \\mathbb{R}$ defined as \n",
    "\n",
    "$$\n",
    "    f(x) =  \\sqrt{2 + x^3 + \\log (1+ x^3)} + \\sin(2 + x^3 + \\log (1+ x^3)) .\n",
    "$$\n",
    "\n",
    "<!-- $$\n",
    "    f(x) = \\sqrt{x^2 + \\exp (x^2)} + \\cos(x^2 + \\exp(x^2)) \\,.\n",
    "$$ -->\n",
    "\n",
    "Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** Write out the chain rule to compute the derivative $f'(x)$ of $f$ at a given $x$. Then, compute the gradient of y with respect to x. \n",
    "You may insert the equations in Markdown directly using LaTeX syntax. \n",
    "\n",
    "**Your answer here**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Implement this function $f$ in code so that it accepts a scalar $x$ and returns $f(x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def my_function(x):\n",
    "    # x is a torch.tensor (this is the PyTorch scalar type)\n",
    "    # Example input `x = torch.tensor(3.14159, requires_grad=True)`\n",
    "    # TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C)** Compute the derivative $f'(x)$ for $x= 1.432$ using PyTorch's automatic differentiation. You do not have to code up your own backward method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Coding up a differentiable module\n",
    "\n",
    "Consider the soft-thresholding function $f_T: \\mathbb{R} \\to \\mathbb{R}$ defined for any $T > 0$ as \n",
    "$$\n",
    "    f_T(y) = \n",
    "    \\begin{cases} \n",
    "        0, & \\text{ if } -T \\le y \\le T \\,, \\\\\n",
    "        (y - T)^2, & \\text{ if } y > T \\,, \\\\\n",
    "        (y + T)^3, & \\text{ if } y < T \\,.\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** Write a function to compute which takes in as arguments $y, T$ and returns the soft-thresholding $f_T(y)$.\n",
    "    Plot this function with $T = 2.81$ in the range $[-15, 15]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def softt(y, T):\n",
    "    \"\"\" `y` is a torch.tensor (i.e., PyTorch's scalar type; same as above), \n",
    "        `T` is a regular Python number (float or int).\n",
    "        return type: torch.tensor\n",
    "    \"\"\"\n",
    "    # TODO: your code here\n",
    "    # HINT: if you write a program with branches, make sure that the output type is always a torch.tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Write a function which computes the derivate $f_T'(y)$ of the soft-thresholding function w.r.t. $y$, as returned by PyTorch. Plot this for $T=2.81$ in the range $[-15, 15]$. \n",
    "\n",
    "**Hint 1**: If you coded up `softt` using branches, you might encounter a situation where the output does not depend on the input. In this case, you will have to appropriately set the `allow_unused` flag. \n",
    "\n",
    "**Hint 2**: When PyTorch returns a derivative of `None`, it actually stands for `0`. If your derivative returns a `None`, you will have to handle this appropriately when plotting the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softt_derivative(y, T): \n",
    "    # TODO: your code here\n",
    "\n",
    "# Test your code\n",
    "a = torch.tensor(1.20, requires_grad=True)\n",
    "print(softt_derivative(a, 3.14))\n",
    "\n",
    "# Plot\n",
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C)** We will now code a differentiable module using `torch.nn.Module`. \n",
    "\n",
    "First, let us extend the definition of \n",
    "the soft-thresholding $f_T$ to vectors by applying the soft-thresholding operation component-wise. \n",
    "\n",
    "Now write a differentiable module which implements the transformation $g_{T}(\\cdot; M): \\mathbb{R}^d \\to \\mathbb{R}^d$ \n",
    "given by \n",
    "$$\n",
    "    g_{T}(x; M) = M^{-1} \\, f_T(Mx) \\,,\n",
    "$$\n",
    "where $M \\in \\mathbb{R}^{d \\times d}$, a symmetric matrix, is a *parameter* of the module. (Recall: parameters maintain state of the module; register a parameter in `torch.nn.Module` by using the `torch.nn.Parameter` wrapper).\n",
    "\n",
    "Supply $T > 0$ and and initial value $M_0 \\in \\mathbb{R}^{d \\times d}$ symmetric to the constructor, while the `forward` method only accepts $x \\in \\mathbb{R}^d$ as an input. \n",
    "\n",
    "You may use the function `create_symmetric_invertible_matrix` to create this matrix `M`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_symmetric_invertible_matrix(dimension):\n",
    "    # return symmetric invertible square matrix of size `dimension` x `dimension`\n",
    "    rng = np.random.RandomState(dimension)\n",
    "    factor = rng.randn(dimension, dimension).astype(np.float32)\n",
    "    return 1e-6 * torch.eye(dimension) + torch.from_numpy(np.matmul(factor, factor.T))\n",
    "    \n",
    "class MatmulSofttMatmulinv(torch.nn.Module):\n",
    "    #### TODO: your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D)** Initialize the module with $T = 2.81$ and $M_0$ using the function `create_symmetric_invertible_matrix`. \n",
    "Use `dimension=5`. Pass in the following vector `x` defined below and compute $g_T(x;M_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dimension = 5\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "print('x:', x)\n",
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E)** For the same vector `x` as defined above, compute and print out the gradient of $\\varphi_T(x; M) = \\|x - g_T(x; M)\\|_2^2$\n",
    "with respect to both $x$ and $M$ using automatic differentiation. Use $T=2.81$ again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Optimizing a multi-layer perceptron\n",
    "In this exercise, you will find the divergent learning rate of a MLP and implement a variant of SGD with parameter averaging. \n",
    "\n",
    "We will start with the dataloading utilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('../data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "f, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, idx in enumerate(np.random.choice(X_train.shape[0], 5)):\n",
    "    ax[i].imshow(X_train[idx], cmap='gray', vmin=0, vmax=255)\n",
    "    ax[i].set_title(f'Label = {y_train[idx]}', fontsize=20)\n",
    "    \n",
    "\n",
    "\n",
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Make them zero mean\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)  # flatten into a (n, d) shape\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) \n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :])\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** Code up a MLP with three hidden layers with 32, 16, and 12 hidden units respectively. We will use a batch size of 8 throughout. The objective function we will use is the multinomial logistic loss, also known in PyTorch as `cross_entropy`. \n",
    "\n",
    "You **may reuse** code from previous labs and demos for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Find the divergent learning rate.\n",
    "\n",
    "**Note**: We changed the data pre-processing (our data no longer has unit variance), which could cause the divergent learning rate to be different from what we had in previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C)** We will implement averaged SGD with an exponentially moving average. \n",
    "In addition to the model parameters $w_t$, we \n",
    "also maintain a separate set of parameters $\\bar w_t$ to serve as an average. The updates of averaged SGD are\n",
    "$$\n",
    "    w_{t+1} = w_t - \\eta g_t \\\\\n",
    "    \\bar w_{t+1} = (1 - \\gamma) \\bar w_t + \\gamma w_{t+1},\n",
    "$$\n",
    "where $\\eta$ is a learning rate, $g_t$ is a (minibatch) stochastic gradient at $w_t$ and $\\gamma \\in (0, 1)$ is an average weight. \n",
    "\n",
    "Some notes:\n",
    "- the update of $w_t$ is identical to the regular SGD method. That is, the averaged parameter $\\bar w_t$ is *not* used during the stochastic gradient updates. \n",
    "- The averaged parameter $\\bar w_t$ is updated on the side and never to be used in model updates. We use $\\bar w_t$ for logging only. \n",
    "\n",
    "Your task is to train the model for $25$ epochs and plot the train/test loss/accuracy for both the unaveraged model $w_t$ as well as the averaged_model $\\bar w_t$ in the same plot. Use a batch size of $8$ and half the divergent learning rate you found in the previous part. We will use the average weight as $\\gamma = 10^{-3}$.\n",
    "\n",
    "\n",
    "**NOTE**: Do not include the logs of the first two passes through the data in the plot. This is because the inital loss is always very large and this tends to drown out the more interesting patterns we observe later on during training. \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: CNN Short Answer\n",
    "Choose **1 from each section** of the following problems to complete.\n",
    "\n",
    "**Section 1 (choose 1)**\n",
    "\n",
    "a. Given a image of size =20 , what is the output shape if it is put through a convolutional neural net with kernel size=3, padding = 2, stride = 4, and number of out_channels = 16? Justify your answer based on the ideas of kernel size, padding, and stride. \n",
    "\n",
    "b. For text data, what does a stride of 2 correspond to? Specifically, what other common metric in text analysis could it be called?\n",
    "\n",
    "c. Do we need a separate minimum pooling layer? Can you replace it with another operation?\n",
    "\n",
    "**Section 2 (choose 1)**\n",
    "\n",
    "c. What are the computational and statistical benefits of a stride larger than 1?\n",
    "\n",
    "d. What is the computational cost of the pooling layer? Assume that the input to the pooling layer is of size $c\\times h\\times w$,the pooling window has a shape of $p_h \\times p_w$ with a padding of $(p_h, p_w)$ and a stride of $(s_h, s_w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "694a2605e6c4cb7e5ecd12cf55a62475b886d9de17fe0247424478eb05601482"
  },
  "kernelspec": {
   "display_name": "Python 3.8.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
