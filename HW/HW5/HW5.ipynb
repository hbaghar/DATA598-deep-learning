{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Embeddings and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise 1: Sentiment Analysis using GloVe Embeddings and Linear Models\n",
    "\n",
    "The goal of this exercise is to perform sentiment analysis using the data from the demo, except using GloVe embeddings. Please see this week's demo for details on the data. \n",
    "\n",
    "\n",
    "Download the data from [here](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data?select=train.tsv.zip). Note that you will need an active Kaggle account to access the data. \n",
    "\n",
    "We will use movie reviews from Rotten Tomatoes. The sentiment labels are:\n",
    "- 0 - negative\n",
    "- 1 - somewhat negative\n",
    "- 2 - neutral\n",
    "- 3 - somewhat positive\n",
    "- 4 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torch.nn.functional import cross_entropy\n",
    "import time\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVeWordEmbedding:\n",
    "    def __init__(self):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding()\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        with open('glove.6B.50d/vec.txt') as f: # use encoding='utf8' on widows\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, np.asarray(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        # \"tokens\" is a list of words\n",
    "        # use as object[tokens]\n",
    "        # map token -> index -> vector\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[np.asarray(indices)]\n",
    "        return vecs\n",
    "    \n",
    "    def __call__(self, tokens):\n",
    "        # Use as object(tokens)\n",
    "        return self.__getitem__(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = GloVeWordEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SentenceId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Phrase  Sentiment\n",
       "SentenceId                                                              \n",
       "1           A series of escapades demonstrating the adage ...          1\n",
       "2           This quiet , introspective and entertaining in...          4\n",
       "3           Even fans of Ismail Merchant 's work , I suspe...          1\n",
       "4           A positively thrilling combination of ethnogra...          3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = '../data/sentiment-analysis-on-movie-reviews/train.tsv'\n",
    "# keep one example per sentence (original data labels each phrase)\n",
    "data = pd.read_csv(filename, sep='\\t').groupby('SentenceId').first()\n",
    "data = data.drop(columns=['PhraseId'])\n",
    "\n",
    "#Remove examples that are empty\n",
    "data = data.replace(\" \", np.nan)\n",
    "data.dropna(axis=0, inplace=True)\n",
    "\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split and featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 2) (1528, 2)\n"
     ]
    }
   ],
   "source": [
    "data = data.sample(frac=1)  # shuffle\n",
    "train_data = data[:7000]\n",
    "test_data = data[7000:]\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clumsy , obvious , preposterous , the movie will likely set the cause of woman warriors back decades .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data[\"Phrase\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\varphi(w)$ denote the embedding of word $w$.\n",
    "Recall that GloVe is a global embedding which does not depend on the context of the word. \n",
    "\n",
    "For a piece of text denoted by words $T = (w_1, \\cdots, w_n)$, we \n",
    "summarize it by the vector \n",
    "$$\n",
    "    \\psi(T) := \\frac{1}{n} \\sum_{i=1}^n \\varphi(w_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(x): # x is pd.Series with text\n",
    "    # TODO: your code here\n",
    "    # Return a matrix with the same number of rows as x\n",
    "    # Each row contains one feature vector for the entire text \n",
    "    matrix = np.zeros((x.shape[0], 50))\n",
    "    \n",
    "    for i,sentence in enumerate(x):\n",
    "        vecs = glove_embedding[sentence.split()]\n",
    "        matrix[i,:] = np.mean(vecs, axis=0)\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = featurize(train_data['Phrase'])\n",
    "y_train = train_data['Sentiment'].values\n",
    "\n",
    "x_test = featurize(test_data['Phrase'])\n",
    "y_test = test_data['Sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a simple logistic regression classifier to test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will reduce the data dimensionality\n",
    "# This step is optional and only perform it if you \n",
    "# find that it helps the test accuracy\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99, random_state=1).fit(x_train)  # keep 99% of the explained variance\n",
    "x_train = pca.transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of C =  2.782559402207126\n",
      "Train accuracy: 0.3954285714285714\n",
      "Test accuracy: 0.39986910994764396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "\n",
    "\n",
    "# TODO: Tune C with cross-validation\n",
    "# clf = LogisticRegression(random_state=0, C=1.0).fit(x_train, y_train)\n",
    "clf = LogisticRegressionCV(Cs = 10, cv=5, random_state=42).fit(x_train,y_train)\n",
    "print(\"Best value of C = \", clf.C_[0])\n",
    "\n",
    "# Re-fit model on entire training data with best value for C\n",
    "model = LogisticRegression(random_state=0, C=clf.C_[0]).fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print('Train accuracy:', (y_train_pred == y_train).mean())\n",
    "print('Test accuracy:', (y_test_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 2: \n",
    "\n",
    "**The exercise**:\n",
    "Perform the same McNemar test as in Part 2 but now compare the same ConvNet from week 2 to a ConvNet with just 1 hidden layer (of hidden size = 16). Use a significance $\\alpha=0.05$. Train each network with SGD for $30$ passes with an appropriate learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('../data', train=True, download=False)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('../data', train=False, download=False)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "# idxs_test = torch.from_numpy(\n",
    "#     np.random.choice(X_test.shape[0], replace=False, size=X_test.shape[0]//10))\n",
    "# X_test, y_test = X_test[idxs_test], y_test[idxs_test]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Normalize them so the pixelwise mean is zero and standard deviation is 1\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :]) / (std[None, :] + 1e-6)\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ConvNet_2layer(torch.nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_ensemble_1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, padding=2), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.conv_ensemble_2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.fc = torch.nn.Linear(7*7*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        out = self.conv_ensemble_1(x)\n",
    "        out = self.conv_ensemble_2(out)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class ConvNet_1layer(torch.nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super().__init__()\n",
    "        # TODO: Fill in for the 1-layer CNN, hidden size = 16\n",
    "        self.conv_ensemble_1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, padding=2), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.fc = torch.nn.Linear(14*14*16, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Your Code Here\n",
    "        x = x.view(-1, 1,28, 28)\n",
    "        out = self.conv_ensemble_1(x)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "# Some utility functions to compute the objective and the accuracy\n",
    "def compute_objective(model, X, y):\n",
    "    score = model(X)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return cross_entropy(input=score, target=y, reduction='mean') \n",
    "\n",
    "def sgd_one_pass(model, X, y, learning_rate, verbose=False):\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "        idx = np.random.choice(X.shape[0])\n",
    "        # compute the objective. \n",
    "        # Note: This function requires X to be of shape (n,d). In this case, n=1 \n",
    "        objective = compute_objective(model, X[idx:idx+1], y[idx:idx+1]) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=model.parameters())\n",
    "        \n",
    "        # perform SGD update. IMPORTANT: Make the update inplace!\n",
    "        for (w, g) in zip(model.parameters(), gradients):\n",
    "            w.data -= learning_rate * g.data\n",
    "      \n",
    "    \n",
    "from tqdm.auto import trange # range + progress bar\n",
    "def sgd_n_passes(model, X_train, y_train, X_val, y_val, n_passes, learning_rate):\n",
    "    for i in trange(n_passes):\n",
    "        sgd_one_pass(model, X_train, y_train, learning_rate)\n",
    "    return compute_prediction_performance(model, X_val, y_val)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_prediction_performance(model, X, y):\n",
    "    # return a boolean vector of the same length as y\n",
    "    # each entry is True if correctly predicted, else False\n",
    "    # TODO: your code here\n",
    "    score = model(X)\n",
    "    preds = torch.argmax(score, dim=1)\n",
    "\n",
    "    return preds == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d66635722d044e48d3f58fd6876f2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = ConvNet_1layer()\n",
    "performance1 = sgd_n_passes(\n",
    "    model1, X_train, y_train, X_test, y_test, n_passes=30, learning_rate=2e-3\n",
    ")\n",
    "# boolean vector of length n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32b65f88ab94e04ab3be83b02afb05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = ConvNet_2layer()\n",
    "performance2 = sgd_n_passes(\n",
    "    model2, X_train, y_train, X_test, y_test, n_passes=30, learning_rate=2.5e-3\n",
    ")\n",
    "# boolean vector of length n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of ConvNet_1layer: 0.8633\n",
      "accuracy of ConvNet_2layer: 0.8722\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of ConvNet_1layer:', performance1.sum().item()/y_test.shape[0])\n",
    "print('accuracy of ConvNet_2layer:', performance2.sum().item()/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N01 = (~performance1 & performance2).sum().item()  \n",
    "N10 = (performance1 & ~performance2).sum().item()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: 9.790139064475348, threshold: 3.8414588206941205\n",
      "Null rejected\n"
     ]
    }
   ],
   "source": [
    "T = (abs(N01-N10)-1)**2/(N10+N01)\n",
    "threshold = scipy.stats.chi2(df=1).ppf(0.95)\n",
    "\n",
    "print(f'Test statistic: {T}, threshold: {threshold}')\n",
    "\n",
    "if T > threshold:\n",
    "    print('Null rejected')\n",
    "else:\n",
    "    print('Failed to reject the null')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86858d5f84902b050d54a6bd9c0fb878f86a1dbd7a1ae5d42a558ee921f88707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
