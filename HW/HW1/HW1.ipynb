{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <center> Homework 1</center>\n",
        "\n",
        "In this homework you will code a simple 1 layer linear regression problem using PyTorch. \n",
        "You can use very similar code to the example shown in Lab 1, but will need to alter\n",
        "it to accomodate linear (instead of logistic) regression. \n",
        "\n",
        "## Instructions\n",
        "\n",
        "Follow the outline code below and fill in code where you see \"\\<YOUR CODE HERE>\\\". \n",
        "Also, answer any questions in the provided \"\\<YOUR ANSWER HERE>\\\" space.\n",
        "Once completed, upload the Jupyter Notebook, a PDF export AND a HTML export version of your Jupyter notebook to Canvas under \"Homework 1\". \n",
        "This homework is due **Friday January 13th at 11:59pm.**\n",
        "\n",
        "# The Data\n",
        "We now consider a list of statements and their sentiment score (provided). Sentiment scores reflect the sentiment of a sentence, with postive values reflecting positive sentiment and negative values reflecting negative sentiment. The magnitude of the score reflects the magnitude of the sentiment (negative or positive). A sentiment score of 0, means the sentence has neutral sentiment. We want to train a single layer linear model using the training data and then test our model on the test data provided below. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'This': 0, 'product': 1, 'is': 2, 'terrible,': 3, 'it': 4, 'broke': 5, 'on': 6, 'the': 7, 'first': 8, 'day': 9, 'Wednesday': 10, 'worst': 11, 'of': 12, 'week.': 13, 'I': 14, 'love': 15, 'going': 16, 'hikes': 17, 'and': 18, 'eating': 19, 'tasty': 20, 'food': 21, 'Blue': 22, 'my': 23, 'favorite': 24, 'color,': 25, 'makes': 26, 'me': 27, 'feel': 28, 'happy': 29, 'Today': 30, 'have': 31, 'to': 32, 'go': 33, 'school': 34, 'am': 35, 'dreading': 36, 'visit': 37, 'dentist.': 38, 'get': 39, 'see': 40, 'circus': 41, 'today,': 42, 'excited.': 43}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# The Data\n",
        "training_data = [(\"This product is terrible, it broke on the first day\".split(), -0.53),\n",
        "        (\"Wednesday is the worst day of the week. \".split(), -0.61),\n",
        "        (\"I love going on hikes and eating tasty food\".split(), 0.41),\n",
        "        (\"Blue is my favorite color, it makes me feel happy\".split(), 0.62),\n",
        "        (\"Today I have to go to school\".split(), -0.02)]\n",
        "\n",
        "test_data = [(\"I am dreading my visit to the dentist.\".split(), -0.31),\n",
        "             (\"I get to see the circus today, I am excited.\".split(), 0.34)]\n",
        "\n",
        "word_to_ix = {}\n",
        "for sent, _ in training_data + test_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "\n",
        "VOCAB_SIZE = len(word_to_ix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Input: ['This', 'product', 'is', 'terrible,', 'it', 'broke', 'on', 'the', 'first', 'day']\n",
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "torch.Size([1, 44])\n",
            "\n",
            "Sample Output: -0.53\n",
            "tensor([[-0.5300]])\n",
            "torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "# Transform sentence into BoW\n",
        "def make_bow_vector(sentence, word_to_ix):\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    for word in sentence:\n",
        "        vec[word_to_ix[word]] += 1\n",
        "    return vec.view(1, -1)\n",
        "\n",
        "\n",
        "def make_target(label):\n",
        "    # return the label as a tensor (instead of a float)\n",
        "    return(torch.FloatTensor([[label]]))\n",
        "\n",
        "# Example\n",
        "print(\"Sample Input:\", training_data[0][0])\n",
        "print(make_bow_vector(training_data[0][0], word_to_ix))\n",
        "print(make_bow_vector(training_data[0][0], word_to_ix).size())\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"Sample Output:\", training_data[0][1])\n",
        "print(make_target(training_data[0][1]))\n",
        "print(make_target(training_data[0][1]).size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the nn.Module for Linear Regression \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class LinearRegression(nn.Module):  \n",
        "\n",
        "    def __init__(self, output_size, input_size):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size,output_size)  # Add one layer of linear regression\n",
        "\n",
        "    def forward(self, bow_vec):\n",
        "        return self.linear(bow_vec)\n",
        "\n",
        "model = LinearRegression(1, len(word_to_ix)) # Input the correct output and input sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So lets train! To do this, choose an appropriate loss function for linear regression from the list provided [here](https://pytorch.org/docs/stable/nn.html#loss-functions). Hint: it should be different than the loss we used in Lab 1. \n",
        "\n",
        "\n",
        "You can use the same optimizer from Lab 1 (torch.optim.SGD) or you can choose\n",
        "a different optimizer from the list [here](https://pytorch.org/docs/stable/optim.html). However, we will need to smaller the learning rate (try 0.01)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(),lr=0.01) # use a learning rate = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we train the model. Use 100 epochs to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Test Values:  [(['I', 'am', 'dreading', 'my', 'visit', 'to', 'the', 'dentist.'], -0.31), (['I', 'get', 'to', 'see', 'the', 'circus', 'today,', 'I', 'am', 'excited.'], 0.34)]\n",
            "tensor([[-0.0368]])\n",
            "tensor([[-0.1063]])\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(100):\n",
        "    for instance, label in training_data:\n",
        "        # Step 1. PyTorch accumulates gradients. DO NOT CHANGE THIS\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Make our BOW vector and also we must wrap the target in a Tensor.\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        target = make_target(label)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        pred = model(bow_vec)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()\n",
        "        loss = loss_function(pred, target)\n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "print(\"True Test Values: \", test_data)\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        pred = model(bow_vec)\n",
        "        print(pred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Questions\n",
        "1. Explain what the output represents. \n",
        "\n",
        "The output represents a sentiment score with positive values indicating positive sentiment and negative values representing negative sentiment and the magnitude reflects how strong the sentiment is.\n",
        "\n",
        "2. Use the \"Run All\" function to run the whole Jupyter Notebook multipe times. Do you get the same results on your test set each time? Why or why not?\n",
        "\n",
        "No, we do not see the same output each time because the weight learning process is not deterministic, i.e. the intial weights are assigned randomly which means subsequent updates will be different based on the intial weights. \n",
        "\n",
        "It is also possible that the learning process has not converged completely, due to which we see different values each time we train the model for 100 epochs. This could be because we have very little training data, additionally, the number of records compared to the number of features in the bag of words representation is also low, or because the learning rate is very high. Further diagnostics would be required/\n",
        "\n",
        "3. Do you think this is a good model for this data? Why or why not?\n",
        "\n",
        "This model is not good for this data, since the outputs are unconstrained. Which means that sentiment scores can take on any arbitrary real value which makes it hard to draw comparisons between scores. Also, the training data scores seem to be constained between -1 and 1, so we should try to make sure that our model is constrained in a similar manner so that we can get meaningful results. \n",
        "\n",
        "Moreover, the model is still a simple linear model that does not capture any complex non-linearities that might be present in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bonus (optional)\n",
        "Can you alter your nn.Module LinearRegression to include two linear layers? \n",
        "Write code for a new nn.Module called \"TwoLayerLinearRegression\" and re-train your model. Then print the predictions of your train model on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class TwoLinearRegression(nn.Module):  \n",
        "\n",
        "    def __init__(self, output_size, input_size):\n",
        "        super(TwoLinearRegression, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size,input_size)\n",
        "        self.linear2 = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, bow_vec):\n",
        "        l1 = self.linear1(bow_vec)\n",
        "        pred = self.linear2(l1)\n",
        "        return pred\n",
        "\n",
        "model = TwoLinearRegression(1, len(word_to_ix)) # Input the correct output and input sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(),lr=0.01) # use a learning rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Test Values:  [(['I', 'am', 'dreading', 'my', 'visit', 'to', 'the', 'dentist.'], -0.31), (['I', 'get', 'to', 'see', 'the', 'circus', 'today,', 'I', 'am', 'excited.'], 0.34)]\n",
            "tensor([[-0.1628]])\n",
            "tensor([[0.0516]])\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(100):\n",
        "    for instance, label in training_data:\n",
        "        # Step 1. PyTorch accumulates gradients. DO NOT CHANGE THIS\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Make our BOW vector and also we must wrap the target in a Tensor.\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        target = make_target(label)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        pred = model(bow_vec)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by calling optimizer.step()\n",
        "        loss = loss_function(pred, target)\n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "print(\"True Test Values: \", test_data)\n",
        "with torch.no_grad():\n",
        "    for instance, label in test_data:\n",
        "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "        pred = model(bow_vec)\n",
        "        print(pred)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data598",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "86858d5f84902b050d54a6bd9c0fb878f86a1dbd7a1ae5d42a558ee921f88707"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
