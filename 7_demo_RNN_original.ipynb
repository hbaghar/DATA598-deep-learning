{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Lecture 7: Sequence Models and Recurrent Neural Networks </h1> </center>\n",
    "<center> Krishna Pillutla, Zaid Harchaoui </center>\n",
    "    <center> Data 598 (Winter 2022), University of Washington </center>\n",
    "\n",
    "In this lecture, we will talk about models which deal with sequential data, and recurrent neural networks in particular.\n",
    "\n",
    "The example in this notebook is based on [this PyTorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) and is inspired by the [D2L book](https://d2l.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Models\n",
    "\n",
    "We will focus on autoregressive models as they capture the essence of modeling sequences. \n",
    "\n",
    "Suppose we have a sequence $x_1, x_2, \\cdots$, for example, a sequence of words which make up a novel. \n",
    "\n",
    "Given a certain part of the sequence, we aim to model the upcoming (and unseen) parts of the sequences. That is, we aim to model \n",
    "$$\n",
    "    x_t \\sim P(\\cdot \\, | \\, x_{t-1}, \\cdots, x_1) .\n",
    "$$\n",
    "![](https://miro.medium.com/max/1734/1*_MrDp6w3Xc-yLuCTbco0xw.png)\n",
    "\n",
    "**Latent Autoregressive Models**:\n",
    "We assume that the data $(x_1, \\cdots, x_{t-1})$ is summarized by a *latent state* or *hidden state* $h_t$. The model is \n",
    "$$\n",
    "    x_t \\sim P(\\cdot | h_t)\n",
    "$$\n",
    "and the hidden state is updated based on the previous hidden state $h_t$ and the data $x_t$ at this time step as \n",
    "$$\n",
    "    h_{t+1} = g(x_t, h_t) \n",
    "$$\n",
    "for some function $g$. \n",
    "\n",
    "The **key modeling assumption** is that $h_t$ is a fixed dimensional vector independent of the length $t$ of the sequence observed so far. \n",
    "\n",
    "![](http://d2l.ai/_images/sequence-model.svg)\n",
    "\n",
    "**Note**: Hidden Markov Models (HMMs) are special instances of latent autoregressive models as well. In this case, the hidden state is updated in a stochastic manner independent of $x_t$, i.e., $h_{t+1} \\sim P_{\\text{transition}}( \\cdot | h_t)$, where $P_{\\text{transition}}$ is called the transition kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a generalization of multi-layer perceptrons (MLPs) to the case of sequential data. Here, the hidden state update function $g(x_t, h_t)$ from above is parameterized with learnable parameters.\n",
    "\n",
    "**MLP**: \n",
    "Recall that a MLP with a single hidden layer is a map $\\psi: \\mathbb{R}^{d_0} \\to \\mathbb{R}^{d_2}$ that can be written as \n",
    "$$\n",
    "    \\psi(x) =\n",
    "    W_2^\\top \\sigma(h_1) + b_2, \\\\\n",
    "     h_1 = W_{1}^\\top x + b_1 , \n",
    "$$\n",
    "where $W_j \\in \\mathbb{R}^{d_{j-1}\\times d_j}$ is a weight matrix and $b_j \\in \\mathbb{R}^{d_j}$ is a bias vector and $\\sigma$ is the *activation function*. \n",
    "\n",
    "\n",
    "**RNN**: In the case of RNNs, the data $x = (x_1, x_2, \\cdots)$ itself is sequential. We start with $h_0 = 0$ and we have,\n",
    "$$\n",
    "    \\psi_t(x) = W_o^\\top h_{t-1} + b_o \\\\\n",
    "    h_{t} = \\sigma(W_{hi}^\\top x_t + W_{hh}^\\top h_{t-1} + b_h)\n",
    "$$\n",
    "![](http://d2l.ai/_images/rnn.svg)\n",
    "In this figure $X_t = x_t$ is the input (a vector of size `input_size`) and $H_t = h_t$ is the hidden state (a vector of size `hidden_size`).\n",
    "\n",
    "**NOTE**: There is difference between a hidden layer of a MLP and a hidden state of a RNN. \n",
    "Both are similar in the sense that they are unobservable, i.e., they are neither an input nor an output. However, the RNN has a temporal component that is absent in a MLP: the input of an RNN itself is sequential. \n",
    "\n",
    "The hidden state $h_{t-1}$ is an inputs to whatever we do at time step $t$. It can be computed by looking at the data $X_1, \\ldots, X_{t-1}$ seen so far.\n",
    "\n",
    "We can have a multi-layer RNN architecture with *hidden RNN layers*. For details, see [Chap. 10.3 of the D2L book](https://d2l.ai/chapter_recurrent-modern/deep-rnn.html): \n",
    "![](https://d2l.ai/_images/deep-rnn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Learning Names from Different Languages\n",
    "\n",
    "We will aim to generate names based on particular language category. \n",
    "\n",
    "We will do this by learning an autoregressive model $P(\\text{name}| \\text{category})$.\n",
    "\n",
    "We will work at the level of *characters*. That is, we will model the text as a sequence of characters and predict the next character using the characters we have seen so far. \n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "Download the data from \n",
    "[this link](https://download.pytorch.org/tutorial/data.zip) \n",
    "and extract it to the current directory.\n",
    "Look into the folder `data/names`, and make sure that 18 files such as `Arabic.txt`, `Chinese.txt`, etc. are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 ['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
    "\n",
    "# Turn a Unicode string to plain ASCII\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def read_lines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "# Build the category_lines dictionary, a list of lines per category\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "for filename in glob.glob('~/data_rnn_demo/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = read_lines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "if n_categories == 0:\n",
    "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
    "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
    "        'the current directory.')\n",
    "\n",
    "print('# categories:', n_categories, all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "print(n_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a RNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                category_size=n_categories):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # (input, category) + hidden - > hidden\n",
    "        self.i2h = torch.nn.Linear(category_size + input_size + hidden_size, hidden_size)\n",
    "        # (input, category) + hidden -> output \n",
    "        self.i2o = torch.nn.Linear(category_size + input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, category, inp, hidden):\n",
    "        # Our model is conditioned on the category as well.\n",
    "        # We implement this conditioning by passing in the category\n",
    "        # together with the input and the hidden state.\n",
    "        input_combined = torch.cat((category, inp, hidden), dim=1)  # concatenate along dimension 1\n",
    "        hidden = relu(self.i2h(input_combined))  # Update the new hidden state\n",
    "        output = self.i2o(input_combined)  # Update the output\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize the hidden state to zeros\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "As we discussed the last time, how we will embed tokens as vectors is quite crucial.\n",
    "Since we are working at the level of characters, and the vocabulary of allowed characters is quite small (`n_letters=59` in this case), \n",
    "we can get away with a one-hot encoding. \n",
    "\n",
    "**Note**: At the word or word-piece level, one must use the embeddings of the kind we discussed the last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot vector for category\n",
    "def category_to_onehot(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def input_to_onehot(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def target_to_indices(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# Make category, input, and target tensors from a random category, line pair\n",
    "def sample_one_example():\n",
    "    category = np.random.choice(all_categories) # sample category\n",
    "    line = np.random.choice(category_lines[category]) # sample line from category\n",
    "    category_tensor = category_to_onehot(category)\n",
    "    input_line_tensor = input_to_onehot(line)\n",
    "    target_line_tensor = target_to_indices(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the RNN process an input?\n",
    "\n",
    "Recurrent neural networks are inherently sequential in nature. The inputs are processed one-by-one, with the hidden state being updated each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "\n",
    "# Sample an example\n",
    "category_tensor, input_line_tensor, target_line_tensor = sample_one_example()\n",
    "\n",
    "# Initialize hidden state\n",
    "hidden = rnn.init_hidden()\n",
    "\n",
    "# Loop over input sentence\n",
    "for i in range(input_line_tensor.shape[0]):\n",
    "    # Update hidden state and make predictions\n",
    "    output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "    # Use this output to predict the token at this point\n",
    "    # ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "We are now ready to train the RNN. Note the extra work we must perform to process a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_rnn_one_pass(rnn, total_num_examples, learning_rate):\n",
    "    avg_loss = 0.0\n",
    "    for i in tqdm(np.arange(total_num_examples)):  # ~2 min per epoch\n",
    "        # sample a random training example\n",
    "        category, input_line, target_line = sample_one_example()\n",
    "        # Process this training example\n",
    "        output, loss = train_one_example(rnn, category, input_line, target_line, learning_rate)\n",
    "        avg_loss = i / (i+1) * avg_loss + loss / (i+1)\n",
    "    return avg_loss\n",
    "\n",
    "def train_one_example(rnn, category_tensor, input_line_tensor, target_line_tensor, learning_rate):\n",
    "    # Perform a single SGD update on the given example\n",
    "    target_line_tensor.unsqueeze_(-1) # batch dimension\n",
    "    hidden = rnn.init_hidden()\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
    "        l = cross_entropy(output, target_line_tensor[i])\n",
    "        loss += l  # loss incrred on predicting the token number i\n",
    "\n",
    "    gradients = torch.autograd.grad(outputs=loss, inputs=rnn.parameters())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for p, g in zip(rnn.parameters(), gradients):\n",
    "            p -= learning_rate * g\n",
    "\n",
    "    # return final output and average loss so far\n",
    "    return output, loss.item() / input_line_tensor.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we sample a single random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20074\n"
     ]
    }
   ],
   "source": [
    "total_num_examples = sum([len(category_lines[c]) for c in all_categories])\n",
    "print(total_num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "We are now ready to train the RNN. We will train it for 5 epochs through our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c4b697078d40f7a4d5da752e22c2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 2.935 \t28.85sec\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72960cfbcff241248691754175b59269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \t 2.57 \t28.64sec\n",
      "Starting epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3afb4803b048669dbd8b21408e609e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t 2.446 \t28.79sec\n",
      "Starting epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb588e9dc884bd6b916f16ceab5e730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 \t 2.364 \t31.15sec\n",
      "Starting epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec35647cb841491ab9332de4073cec32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \t 2.31 \t33.07sec\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_letters, 128, n_letters)\n",
    "learning_rate = 0.0005\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(5):\n",
    "    t1 = time.time()\n",
    "    print(f'Starting epoch {epoch}')\n",
    "    avg_loss = train_rnn_one_pass(rnn, total_num_examples, learning_rate)\n",
    "    print(epoch+1, '\\t', round(avg_loss, 3), \n",
    "          f'\\t{round(time.time()-t1, 2)}sec')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new names from the network\n",
    "\n",
    "To sample we give the network a letter and ask what the next one is,\n",
    "feed that in as the next letter, and repeat until the EOS token.\n",
    "\n",
    "-  Create tensors for input category, starting letter, and empty hidden\n",
    "   state\n",
    "-  Create a string ``output_name`` with the starting letter\n",
    "-  Up to a maximum output length,\n",
    "\n",
    "   -  Feed the current letter to the network\n",
    "   -  Get the next letter from highest output, and next hidden state\n",
    "   -  If the letter is EOS, stop here\n",
    "   -  If a regular letter, add to ``output_name`` and continue\n",
    "\n",
    "-  Return the final name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Generate from a category and starting letter\n",
    "@torch.no_grad()\n",
    "def generate_name_greedy(category, start_letter):\n",
    "    category_tensor = category_to_onehot(category)\n",
    "    inp = input_to_onehot(start_letter)\n",
    "    hidden = rnn.init_hidden()\n",
    "\n",
    "    output_name = start_letter\n",
    "\n",
    "    for i in range(max_length):\n",
    "        # inp[-1] => pass the last character through the RNN\n",
    "        output, hidden = rnn(category_tensor, inp[-1], hidden)\n",
    "        # output: (1, n_letters) \n",
    "        topi = output[0].argmax()\n",
    "        if topi == n_letters - 1:  # EOS token\n",
    "            break\n",
    "        else:\n",
    "            # add new\n",
    "            letter = all_letters[topi]\n",
    "            output_name += letter\n",
    "        inp = input_to_onehot(output_name) \n",
    "        \n",
    "    return output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n"
     ]
    }
   ],
   "source": [
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Marer'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_name_greedy('French', start_letter='M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sample from the network for names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Generate from a category and starting letter\n",
    "@torch.no_grad()\n",
    "def generate_name_sample(category, start_letter, temperature=0.5):\n",
    "    category_tensor = category_to_onehot(category)\n",
    "    inp = input_to_onehot(start_letter)\n",
    "    hidden = rnn.init_hidden()\n",
    "\n",
    "    output_name = start_letter\n",
    "\n",
    "    for i in range(max_length):\n",
    "        # inp[-1] => pass the last character through the RNN\n",
    "        output, hidden = rnn(category_tensor, inp[-1], hidden)\n",
    "        # output: (1, n_letters); 1 is the batch_size\n",
    "        probabilities = softmax(output[0]/temperature, dim=0)\n",
    "        next_letter = torch.multinomial(probabilities, 1)[0].item()\n",
    "        if next_letter == n_letters - 1:  # EOS token\n",
    "            break\n",
    "        else:\n",
    "            letter = all_letters[next_letter]\n",
    "            output_name += letter\n",
    "        inp = input_to_onehot(output_name)\n",
    "        \n",
    "    return output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n"
     ]
    }
   ],
   "source": [
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kouea'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_name_sample('Japanese', start_letter='K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 1: RNN for Classification\n",
    "In this exercise, we run train a RNN-based classifier which can classify names to categories (i.e., language of origin). \n",
    "In ML terms, the input is a name (which we treat as a sequence of characters) and the output is its category. \n",
    "\n",
    "Details:\n",
    "- Use the `RNNForClassification` below. This is a simplification of the RNN class we used above. Note that the forward method does not take the category as an input anymore, since this is the output we would like to predict.\n",
    "- We will run through the entire sequence with the RNN to compute the loss once. In particular, the `train_one_example` function from above will now be modified as:\n",
    "```\n",
    "# Step 1: obtain the *final* output\n",
    "for i in range(input_line_tensor.shape[0]):\n",
    "        output, hidden = rnn(input_line_tensor[i], hidden)\n",
    "# Step 2: compute the loss based on the *final* output\n",
    "# output is of shape (n_categories,) = (18,)\n",
    "loss = cross_entropy(output, category)\n",
    "# Note: pass in the raw category as a LongTensor of length 1. In the notebook above, we used a one-hot encoding, which we called as `category_tensor`\n",
    "```\n",
    "\n",
    "Deliverables:\n",
    "- What are the differences between the `RNN` class above and `RNNForClassification` class below? Why do you think we have these differences?\n",
    "- Separate out a validation set from the training set as 20% of names for each of the categories. \n",
    "- Find the divergent learning rate. Hint: you will have to train for a whole epoch to test for divergence. The loss might appear to be going down but then it could suddenly explode. RNNs are a not as robust to learning rates as MLPs. Practitioners use various tricks such as gradient clipping to deal with these issues. \n",
    "- Train the model for 5 epochs with quarter the divergent learning rate.\n",
    "- Report the training and validation accuracy at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNForClassification(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=n_categories):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # input + old_hidden -> new_hidden\n",
    "        self.i2h = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        # input + hidden -> output \n",
    "        self.i2o = torch.nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        input_combined = torch.cat((inp, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 2: RNNs for Sentiment Analysis\n",
    "\n",
    "The goal of this exercise is to perform sentiment analysis with a recurrent neural network. \n",
    "\n",
    "**Note**: This is an advanced exercise which brings together multiple concepts we have learned across all of the course. You might find it easier to attempt this after working out Bonus Exercise 1 above.\n",
    "\n",
    "**Data**: We will use _movie review_ data from _Rotten Tomatoes_. Please see the demo of week 6 for details on the data. Download the data from [here](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data?select=train.tsv.zip). Also download the test set from the same page.\n",
    "Note that you will need an active Kaggle account to access the data.\n",
    "\n",
    "We will use . The sentiment labels are:\n",
    "- 0 - negative\n",
    "- 1 - somewhat negative\n",
    "- 2 - neutral\n",
    "- 3 - somewhat positive\n",
    "- 4 - positive\n",
    "\n",
    "**Model**: Use a word-level recurrent neural network built on the GloVe embeddings. See the lab from week 6 for details on how to obtain the GloVe embeddings for words. \n",
    "- The input `inp` to the forward method is the GloVe embedding of the input word. \n",
    "- The output dimension is 5, corresponding to the sentiment labels. \n",
    "- Use a hidden state of dimension 128. \n",
    "- Similar to the `RNNForClassification` above, compute the `cross_entropy` loss once for the entire sequence of words. \n",
    "\n",
    "**Optimization**: Find the divergent learning rate. Use 1/4th of the divergent learning to optimize the model with SGD. Plot the train and test loss and accuracy over the course of training. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "694a2605e6c4cb7e5ecd12cf55a62475b886d9de17fe0247424478eb05601482"
  },
  "kernelspec": {
   "display_name": "Python 3.8.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
