{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  <h1> Lab 4: Batch Normalization and Text CNN's </h1> </center> \n",
    "<center>  <h1> SOLUTIONS </h1> </center> \n",
    "<center> Jillian Fisher, Zaid Harchaoui </center>\n",
    "    <center> Data 598 (Winter 2023), University of Washington </center>\n",
    "    <center>  </center>\n",
    "\n",
    "In this lab, we will study batch normalization and CNN's for text. \n",
    "For simplicity, we will stick to multilayer perceptrons (MLPs) for batch normalization\n",
    "but the same ideas extend directly to convolutional neural networks.\n",
    "\n",
    "\n",
    "This notebook is inspired by the [D2L book](https://d2l.ai/), http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/11/27/Understanding-Convolutions-In-Text/, https://tzuruey.medium.com/using-convolution-neural-networks-to-classify-text-in-pytorch-3b626a42c3ca,  and adapted from lecture material created by Krishna Pillutla.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization in linear regression: An optimization viewpoint\n",
    "\n",
    "Recall that we always normalize our features to be zero mean and unit variance \n",
    "(or something similar) on our training set.\n",
    "\n",
    "Let us stack our input data $x_1, \\cdots, x_n \\in \\mathbb{R}^d$ into a matrix $X \\in \\mathbb{R}^{n \\times d}$. Denote the coordinate-wise mean and standard deviation as \n",
    "$$\n",
    "    \\mu^{(j)} = \\frac{1}{n} \\sum_{i=1}^n x_i^{(j)} \\,, \\quad\n",
    "    \\sigma^{(j)} = \\sqrt{\\frac{1}{n} (x_i^{(j)} - \\mu^{(j)})^2} \\,,\n",
    "$$\n",
    "for $j = 1, \\cdots, d$. \n",
    "We can simply write the standardization as\n",
    "$$\n",
    "    \\hat x_i = \\frac{1}{\\sigma}(x_i - \\mu)\n",
    "$$\n",
    "where the operations are applied elementwise. \n",
    "We have applied this transformation to our data in each lab so far. \n",
    "\n",
    "\n",
    "Consider the linear prediction model \n",
    "$\n",
    "f(x) = w^\\top x + b\n",
    "$\n",
    "to predict outputs $y \\approx f(x)$ from inputs $x$. Given $n$ input-output samples $(x_1, y_1), \\ldots, (x_n, y_n) \\in \\mathbb{R}^d \\times \\mathbb{R}$, minimizing the least square loss over the training data reads\n",
    "$$\n",
    "\\min_{w,b} \\frac{1}{n}\\sum_{i=1}^n  (y_i - w^\\top x_i - b)^2.\n",
    "$$\n",
    "\n",
    "**Mean Normalization (Homework exercise)**:\n",
    "Show that minimizing in $b$ amounts to center both inputs and outputs leading to \n",
    "$$\n",
    "\\min_{w} f(w) = \\frac{1}{n}\\sum_{i=1}^n  (\\widetilde y_i - w^\\top \\hat x_i)^2.\n",
    "$$\n",
    "where $\\widetilde y$ and $\\hat x$ are respectively the centered outputs and inputs.\n",
    "\n",
    "\n",
    "Recall now that from an optimization point of view, the step-size of a gradient step is controlled by the smoothness of the function, i.e. the Lipschitz-continuity of its gradient, which (for twice continuously differentiable functions) can simply computed as the largest eigenvalue of the Hessian matrix. \n",
    "\n",
    "**Variance Normalization**:\n",
    "Similar to above, one can show that \n",
    "after normalizing the inputs, the objective  $f(w) = \\frac{1}{n}\\sum_{i=1}^n  (\\widetilde y_i - w^\\top \\widehat x_i)^2$ has a smoothness constant no bigger than d.\n",
    "\n",
    "In other words, feature normalization allows one to use a standard learning rate, invariant to the scale of the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "We standardize our inputs in neural networks as well, but that only ensures that inputs to the first layer are normalized. What about inputs to subsequent layers?\n",
    "\n",
    "**What is batch norm?**\n",
    "\n",
    "Batch normalization is a technique which generalizes this standardization operation to ensure normalization of inputs in to an intermediate layer. \n",
    "\n",
    "In the linear model case, the mean and variance of our features do not change (since the data is fixed), therefore precomputing them once suffices. \n",
    "However, the inputs to hidden layers in neural networks changes over time. \n",
    "Batch norm, therefore, computes the mean and variance over a *minibatch of data*.\n",
    "\n",
    "The batch norm layer gives the network two *trainable* parameters $\\beta, \\gamma$\n",
    "which are learned over the course of training. If $\\hat x_i$ is the normalized \n",
    "version of input $x_i$ to a batch norm layer, the output of the batch norm layer \n",
    "is \n",
    "$\\gamma \\hat x_i + \\beta$.\n",
    "\n",
    "**Note**: Batch norm requires large minibatch sizes such as 64 or 128 to work. The mean and standard deviation estimates computed from smaller batch sizes will be too noisy otherwise.\n",
    "\n",
    "The pseudocode is given below. \n",
    "\n",
    "<img src=\"https://melfm.github.io/posts/2018-08-Understanding-Normalization/img/batch_norm_forward.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch norm in action\n",
    "Our first step is to notice that batch norm behaves differently at train and test times. \n",
    "\n",
    "At test time, there is no reason for a model to receive a large enough minibatch of inputs (think about the deep networks running on your smart phone).\n",
    "Batch norm does not work if the minibatch size is too small. \n",
    "\n",
    "A solution to this issue is to estimate a running mean and standard deviation of the data encountered during training and use this as a proxy at test time. \n",
    "\n",
    "As a result, we need to tell PyTorch to switch from training to evaluation mode and vice-versa using `model.train()` and `model.eval()`, where `model` is a `torch.nn.Module`. \n",
    "\n",
    "**Part 1: Batch norm in action**:\n",
    "Let us visualize how batch norm behaves differently at train and test times on a multilayer perceptron (MLP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.nn.functional import cross_entropy, relu\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PyTorch's `BatchNorm1d` function. It is called so because the \n",
    "input a batch of vectors in $\\mathbb{R}^d$ (in the form of a matrix of size $n\\times d$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass in input dimensionality (it needs to create mean/std vectors of this dimension)\n",
    "batch_norm_layer = torch.nn.BatchNorm1d(num_features=10)\n",
    "\n",
    "# This is an instance of torch.nn.Module; we can use it as we would a module\n",
    "isinstance(batch_norm_layer, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input mean:\n",
      "tensor([ 9.3583, 13.6268,  1.7787,  2.8271,  7.5137, 13.0383, 11.1088, 19.5494,\n",
      "         9.4111, 12.9476])\n",
      "Input std:\n",
      "tensor([3.6872, 3.6393, 0.2201, 1.5782, 2.5794, 5.5826, 6.1900, 5.1627, 1.4695,\n",
      "        2.8457])\n"
     ]
    }
   ],
   "source": [
    "# Let us create some synthetic data with a nonzero mean\n",
    "mean_vector = 20 * torch.rand(10)  # non-zero mean\n",
    "std_vector = 10 * torch.rand(10)\n",
    "\n",
    "def get_random_data():\n",
    "    return std_vector[None] * torch.randn(25, 10) + mean_vector[None]  # mean = mean_vector\n",
    "random_data = get_random_data()\n",
    "\n",
    "## Let us go to evaluation mode\n",
    "batch_norm_layer.eval()\n",
    "\n",
    "# print mean and variance of the data\n",
    "print('Input mean:')\n",
    "print(random_data.mean(dim=0))\n",
    "\n",
    "print('Input std:')\n",
    "print(random_data.std(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output mean:\n",
      "tensor([ 9.3582, 13.6268,  1.7787,  2.8271,  7.5137, 13.0382, 11.1087, 19.5493,\n",
      "         9.4110, 12.9476], grad_fn=<MeanBackward1>)\n",
      "Output std:\n",
      "tensor([3.6872, 3.6393, 0.2201, 1.5782, 2.5794, 5.5826, 6.1900, 5.1627, 1.4695,\n",
      "        2.8457], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let us run our data through the batch norm layer and register its output\n",
    "output = batch_norm_layer(random_data)\n",
    "\n",
    "print('Output mean:')\n",
    "print(output.mean(dim=0))\n",
    "\n",
    "print('Output std:')\n",
    "print(output.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not observe any difference because batch norm in evaluation mode uses \n",
    "historical statistics of mean and std. But since this batch norm layer was just initialized, \n",
    "it has no history to go by.\n",
    "\n",
    "Hence, it uses its defaults: $\\mu = 0$ and $\\sigma = 1$, which makes this batch norm layer just the identity map.\n",
    "\n",
    "Let us switch to the train mode now. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output mean:\n",
      "tensor([ 2.8610e-08, -7.6294e-08,  2.2888e-07, -3.8147e-08, -6.1989e-08,\n",
      "        -1.5259e-07,  1.0967e-07,  1.1444e-07,  8.3923e-07, -2.7657e-07],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "Output std:\n",
      "tensor([1.0206, 1.0206, 1.0205, 1.0206, 1.0206, 1.0206, 1.0206, 1.0206, 1.0206,\n",
      "        1.0206], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_norm_layer.train()  # switch to train mode\n",
    "\n",
    "output = batch_norm_layer(random_data)\n",
    "\n",
    "print('Output mean:')\n",
    "print(output.mean(dim=0))\n",
    "\n",
    "print('Output std:')\n",
    "print(output.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voilà! It works as expected. Let us run a few more random samples and investigate the evaluation mode further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input mean:\n",
      "tensor([ 9.9646, 16.0109,  1.8047,  3.2935,  6.6255, 13.3679,  8.3322, 19.1193,\n",
      "         8.8255, 12.2329])\n",
      "Input std:\n",
      "tensor([3.8378, 3.5806, 0.2178, 1.6437, 2.4663, 5.0291, 6.6465, 7.5992, 1.4084,\n",
      "        2.6209])\n",
      "Output mean:\n",
      "tensor([ 0.0882,  0.2729,  0.1629,  0.4320,  0.1669,  0.1631, -0.1770,  0.1296,\n",
      "        -0.1818, -0.2027], grad_fn=<MeanBackward1>)\n",
      "Output std:\n",
      "tensor([1.0888, 0.9206, 1.0158, 0.9874, 0.8529, 0.9932, 0.9495, 0.9586, 0.8898,\n",
      "        0.9480], grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):  # so that batch norm updates its internal statistics\n",
    "    random_data = get_random_data()\n",
    "    output = batch_norm_layer(random_data)\n",
    "    \n",
    "batch_norm_layer.eval()  # switch to eval mode\n",
    "\n",
    "random_data = get_random_data()\n",
    "# print mean and variance of the data\n",
    "print('Input mean:')\n",
    "print(random_data.mean(dim=0))\n",
    "\n",
    "print('Input std:')\n",
    "print(random_data.std(dim=0))\n",
    "\n",
    "output = batch_norm_layer(random_data)\n",
    "\n",
    "print('Output mean:')\n",
    "print(output.mean(dim=0))\n",
    "\n",
    "print('Output std:')\n",
    "print(output.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is much better normalized now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Batch norm + MLPs\n",
    "Just as in the convex case, batch norm helps with learning rates in the nonconvex case as well, especially when the networks are very deep. \n",
    "\n",
    "We start with a simple MLP module. Please read it carefully and understand what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, \n",
    "                 num_hidden_layers, hidden_width, \n",
    "                 use_batch_norm=False):\n",
    "        ## pass in how many hidden layers to use and width of each layer\n",
    "        ## We will use the same width for all hidden layers\n",
    "        super().__init__()  # call constructor of a super class\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        # Construct linear layers: use ModuleList to hold a list of submodules\n",
    "        self.hidden_layers = torch.nn.ModuleList([torch.nn.Linear(input_dim, hidden_width)])\n",
    "         # input -> hidden\n",
    "        \n",
    "        for i in range(num_hidden_layers-1):\n",
    "            # hidden i -> hidden i+1\n",
    "            self.hidden_layers.append(torch.nn.Linear(hidden_width, hidden_width))\n",
    "            \n",
    "        # hidden -> output\n",
    "        self.hidden_to_output = torch.nn.Linear(hidden_width, output_dim)\n",
    "            \n",
    "        # construct batch norm layers\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        if use_batch_norm:\n",
    "            print('using batch norm')\n",
    "            self.batch_norm_layers = torch.nn.ModuleList(\n",
    "                [torch.nn.BatchNorm1d(hidden_width)\n",
    "                 for _ in range(num_hidden_layers)]\n",
    "            )\n",
    "        else:\n",
    "            print('not using batch norm')\n",
    "            \n",
    "    def forward(self, x,):\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            # apply self.hidden_layers[i] to x\n",
    "            x = self.hidden_layers[i](x)\n",
    "            # apply relu to x\n",
    "            x = relu(x)\n",
    "            # apply batch norm if required\n",
    "            if self.use_batch_norm:\n",
    "                x = self.batch_norm_layers[i](x)\n",
    "        # hidden -> output\n",
    "        return self.hidden_to_output(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using batch norm\n",
      "MultiLayerPerceptron(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (hidden_to_output): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (batch_norm_layers): ModuleList(\n",
      "    (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerPerceptron(input_dim=784, output_dim=10, hidden_width=64,  \n",
    "                              num_hidden_layers=3, \n",
    "                              use_batch_norm=True)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load our FashionMNIST dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('./data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "# idxs_test = torch.from_numpy(\n",
    "#     np.random.choice(X_test.shape[0], replace=False, size=X_test.shape[0]//10))\n",
    "# X_test, y_test = X_test[idxs_test], y_test[idxs_test]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Normalize them so the pixelwise mean is zero and standard deviation is 1\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)  # flatten into a (n, d) shape\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :]) / (std[None, :] + 1e-6)\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the training utilities. \n",
    "\n",
    "**Note**: We must pay extra care to ensure that the model is in training or evaluation mode when appropriate.\n",
    "Batch norm behaves differently at train time and test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_objective(model, X, y):\n",
    "    \"\"\" Compute the multinomial logistic loss. \n",
    "        model is a module\n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    # send \n",
    "    score = model(X)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return cross_entropy(input=score, target=y, reduction='mean') \n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(model, X, y):\n",
    "    \"\"\" Compute the classification accuracy\n",
    "        ws is a list of tensors of consistent shapes \n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    is_train = model.training  # if True, model is in training mode\n",
    "    model.eval()  # use eval mode for accuracy\n",
    "    score = model(X)\n",
    "    predictions = torch.argmax(score, axis=1)  # class with highest score is predicted\n",
    "    if is_train:  # switch back to train mode if appropriate\n",
    "        model.train()\n",
    "    return (predictions == y).sum() * 1.0 / y.shape[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logs(model, verbose=False):\n",
    "    is_train = model.training  # if True, model is in training mode\n",
    "    model.eval()  # switch to eval mode\n",
    "    train_loss = compute_objective(model, X_train, y_train)\n",
    "    test_loss = compute_objective(model, X_test, y_test)\n",
    "    train_accuracy = compute_accuracy(model, X_train, y_train)\n",
    "    test_accuracy = compute_accuracy(model, X_test, y_test)\n",
    "    if verbose:\n",
    "        print(('Train Loss = {:.3f}, Train Accuracy = {:.3f}, ' + \n",
    "               'Test Loss = {:.3f}, Test Accuracy = {:.3f}').format(\n",
    "                train_loss.item(), train_accuracy.item(), \n",
    "                test_loss.item(), test_accuracy.item())\n",
    "    )\n",
    "    if is_train:  # switch back to train mode if appropriate\n",
    "        model.train()\n",
    "    return (train_loss, train_accuracy, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the minibatch SGD function (copied over from a previous lab). \n",
    "Note again that we must pay special care to making sure the model is in training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_sgd_one_pass(model, X, y, learning_rate, batch_size, verbose=False):\n",
    "    model.train()\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    num_updates = int(round(num_examples / batch_size))\n",
    "    for i in range(num_updates):\n",
    "        idxs = np.random.choice(X.shape[0], size=(batch_size,)) # draw `batch_size` many samples\n",
    "        model.train()  # make sure we are in train mode\n",
    "        # compute the objective. \n",
    "        objective = compute_objective(model, X[idxs], y[idxs]) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=model.parameters())\n",
    "        \n",
    "        # perform SGD update. IMPORTANT: Make the update inplace!\n",
    "        with torch.no_grad():\n",
    "            for (w, g) in zip(model.parameters(), gradients):\n",
    "                w -= learning_rate * g\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal now is to find the divergent learning rates \n",
    "with and without batch norm. We use a fixed batch size of 32\n",
    "\n",
    "Use a hidden width of 64 throughout. \n",
    "Vary the depth of the network\n",
    "(i.e., the number of hidden layers)\n",
    "between 1 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not using batch norm\n",
      "Train Loss = 2.305, Train Accuracy = 0.098, Test Loss = 2.306, Test Accuracy = 0.100\n",
      "1.4610244231938492\n",
      "1.459454987796917\n",
      "1.4601824636020182\n",
      "1.4601957181051841\n",
      "1.4596348277282227\n",
      "1.459758722001808\n",
      "1.459724082906664\n",
      "1.459888446296574\n",
      "1.4605523654208508\n",
      "1.4602426229716317\n",
      "1.4596335457293244\n",
      "1.4600472015893164\n",
      "1.4596916614444821\n",
      "1.4595547688885508\n",
      "1.4595775951869876\n",
      "1.459566127352911\n",
      "1.459253654706653\n",
      "1.459558534182161\n",
      "1.459748457194877\n",
      "1.459541548060624\n",
      "1.458791563649635\n",
      "1.459197666100485\n",
      "1.4599110055244129\n",
      "1.4600266489496583\n",
      "1.4593013398284782\n",
      "1.4600307546099458\n",
      "1.4593770488230888\n",
      "1.45895676442173\n",
      "1.459398770227837\n",
      "1.4596061542967302\n",
      "1.459977881629981\n",
      "1.4598174398982913\n",
      "1.4596942845498477\n",
      "1.4601149311301247\n",
      "1.4594260978013025\n",
      "1.4593885459730447\n",
      "1.4591177991081752\n",
      "1.4600294882359568\n",
      "1.4593796413550661\n",
      "1.4598271469078352\n",
      "1.4590010752881084\n",
      "1.4591937226424145\n",
      "1.459816625884016\n",
      "1.4602565087200026\n",
      "1.4587503487837823\n",
      "1.4588108040709156\n",
      "1.459703557086519\n",
      "1.4600616629423249\n",
      "1.459387100454345\n",
      "1.4594370732924056\n",
      "1.4599063571318573\n",
      "1.4595751004970792\n",
      "1.4594987695357289\n",
      "1.4597274741121857\n",
      "1.45933482059223\n",
      "1.4590049783753012\n",
      "1.4589578220492003\n",
      "1.459922751293793\n",
      "1.459364621016713\n",
      "1.459121033935423\n",
      "1.4600089013846544\n",
      "1.4593762350808461\n",
      "1.4595623301931198\n",
      "1.4591985160489374\n"
     ]
    }
   ],
   "source": [
    "num_hidden_layers = 25\n",
    "\n",
    "# no batch norm\n",
    "model = MultiLayerPerceptron(input_dim=784, output_dim=10, hidden_width=64,  \n",
    "                              num_hidden_layers=num_hidden_layers, \n",
    "                              use_batch_norm=False)\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "logs = []\n",
    "\n",
    "logs.append(compute_logs(model, verbose=True))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for _ in range(batch_size*2):  # run multiple passes because our sub-sampled dataset is too small\n",
    "    model = minibatch_sgd_one_pass(model, X_train, y_train, learning_rate, \n",
    "                                   batch_size=batch_size, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using batch norm\n",
      "Train Loss = 2.303, Train Accuracy = 0.100, Test Loss = 2.304, Test Accuracy = 0.100\n",
      "1.3052448224245976\n",
      "1.0041311674302094\n",
      "1.1985350510943467\n",
      "1.132449850165345\n",
      "0.9940929182861032\n",
      "0.9751480897977858\n",
      "0.9326149993757314\n",
      "0.8867976356882423\n",
      "0.8971497463342714\n",
      "0.8768544997353401\n",
      "0.8782271612311081\n",
      "0.8719234527397317\n",
      "0.8210365360066042\n",
      "0.8397398748234253\n",
      "0.7882526543423359\n",
      "0.7943221293121556\n",
      "0.7916193347101231\n",
      "0.74583143826128\n",
      "0.7533117222659959\n",
      "0.724431658797501\n",
      "0.6911159795570911\n",
      "0.744039916609071\n",
      "0.6960081046017008\n",
      "0.9596366051768203\n",
      "0.8005560476055051\n",
      "1.102533185476845\n",
      "1.0789373949179668\n",
      "1.0290348359022192\n",
      "1.0185691574843514\n",
      "0.9911090865461109\n",
      "0.9558448767449422\n",
      "0.9458174577526165\n"
     ]
    }
   ],
   "source": [
    "# with batch norm\n",
    "model = MultiLayerPerceptron(input_dim=784, output_dim=10, hidden_width=64,  \n",
    "                              num_hidden_layers=num_hidden_layers, \n",
    "                              use_batch_norm=True)\n",
    "\n",
    "\n",
    "learning_rate = 1.28\n",
    "\n",
    "logs = []\n",
    "\n",
    "logs.append(compute_logs(model, verbose=True))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for _ in range(batch_size):  # run multiple passes because our sub-sampled dataset is too small\n",
    "    model = minibatch_sgd_one_pass(model, X_train, y_train, learning_rate, \n",
    "                                   batch_size=batch_size, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redo the last part with depths of $10$ and $25$, and $50$. What do you observe?\n",
    "\n",
    "**NOTE**: the larger models (depth > 20) get very slow. Use a cloud computing resource if it is too slow on your laptop.\n",
    "\n",
    "**Hint**: If some of the models at these depths get stuck or suddenly diverge to infinity/NaN, know that it is an often encountered problem with very deep networks. A combination of many factors can overcome this issue: careful initialization and tuning of learning rates as well as dropout. The most popular fix, however, is to use residual networks, know as _ResNets_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring CNN for Text\n",
    "Now, we will explore convoluation networks more thoroughly using the text example from the demo. \n",
    "\n",
    "First, we download the data and tokenizer the input. We will again use the Spacy tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "# Install docopt and spacy\n",
    "%%capture\n",
    "# If this give an error, in your terminal activate the data598 env \"conda activate data598\" and then install each\n",
    "#package seperately: \"pip install spacy\", \"conda install docopt\"\n",
    "import pip\n",
    "!pip install spacy\n",
    "import spacy\n",
    "\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Input/Label\n",
      "Input: What films featured the character Popeye Doyle ?\n",
      "Label: ENTY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['ABBR', 'DESC', 'ENTY', 'HUM', 'LOC', 'NUM'], dtype='<U4')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download Text REtrietest Conference (TREC) Question Classifiction data (https://cogcomp.seas.upenn.edu/Data/QA/QC/)\n",
    "import requests\n",
    "\n",
    "# Training Data\n",
    "URL = \"https://cogcomp.seas.upenn.edu/Data/QA/QC/train_1000.label\"\n",
    "TREC_train_data = str(requests.get(URL).content).split(\"\\\\n\")\n",
    "train_label = []\n",
    "train_input = []\n",
    "for i, t in enumerate(TREC_train_data[:-1]):\n",
    "    if i ==0:\n",
    "        train_label.append(t.split(\":\")[0][2:])\n",
    "    else:\n",
    "      train_label.append(t.split(\":\")[0])\n",
    "    train_input.append(\" \".join(t.split(\":\")[1].split(\" \")[1:]))\n",
    "\n",
    "train={\"input\":train_input, \"label\":train_label}\n",
    "\n",
    "# Test Data\n",
    "URL = \"https://cogcomp.seas.upenn.edu/Data/QA/QC/TREC_10.label\"\n",
    "TREC_test_data = str(requests.get(URL).content).split(\"\\\\n\")\n",
    "test_label = []\n",
    "test_input = []\n",
    "for i, t in enumerate(TREC_test_data[:-1]):\n",
    "    if i ==0:\n",
    "        test_label.append(t.split(\":\")[0][2:])\n",
    "    else:\n",
    "      test_label.append(t.split(\":\")[0])\n",
    "    test_input.append(\" \".join(t.split(\":\")[1].split(\" \")[1:]))\n",
    "test={\"input\":test_input, \"label\":test_label}\n",
    "\n",
    "print(\"Example of Input/Label\")\n",
    "print(\"Input:\", train_input[1])\n",
    "print(\"Label:\", train_label[1])\n",
    "\n",
    "np.unique(list(train['label'])+list(test['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Label: 2\n",
      "Nclasses: 6\n"
     ]
    }
   ],
   "source": [
    "# Change string label to integer label\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train['label']+test['label'])\n",
    "train['categorical_label'] = le.transform(train['label'])\n",
    "test['categorical_label'] = le.transform(test['label'])\n",
    "print(\"New Label:\", train['categorical_label'][1])\n",
    "print(\"Nclasses:\", len(np.unique(list(train['categorical_label'])+list(test['categorical_label']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of One word Vector: 96\n",
      "Word Vector Example:  [array([ 0.05669461,  0.14431228, -0.09996024,  0.24405436,  0.17144223,\n",
      "       -0.00698099,  0.5538776 , -0.04801345,  0.10472064, -0.0724301 ,\n",
      "        0.46154702,  0.6193586 , -0.2742111 , -0.36640358, -0.41472915,\n",
      "       -0.3367687 , -0.80017823,  0.01405566,  0.37896937, -0.26699197,\n",
      "        0.06921677,  0.1596782 ,  0.4810588 , -0.59376097,  0.54818946,\n",
      "        0.47652596,  0.55965155, -0.21928822,  0.08473992,  0.5288323 ,\n",
      "       -0.47222984, -0.03617568, -0.03886646, -0.4117626 , -0.05733662,\n",
      "        0.00477944, -0.20796035,  0.08200786,  0.00801046,  0.29752997,\n",
      "       -0.22893679,  0.33473495,  0.16553667, -0.12274137, -0.3228015 ,\n",
      "       -0.28731245,  0.04599106,  0.05031565,  0.18770975,  0.06129264,\n",
      "       -0.1034981 , -0.06582367,  0.57545704,  0.15352717, -0.22472565,\n",
      "       -0.04059392,  0.11420984,  0.40521163,  0.3088222 , -0.2678545 ,\n",
      "       -0.24282809,  0.06950298, -0.37544304, -0.21284604,  0.14027752,\n",
      "       -0.06516393, -0.03447382, -0.03573708,  0.52523637, -0.59921193,\n",
      "        0.11967669,  0.18832311,  0.2369412 , -0.5269017 , -0.05410582,\n",
      "       -0.3671922 , -0.65470433, -0.14447623, -0.3175741 , -0.33863568,\n",
      "        0.10537602, -0.18030086, -0.6207502 , -0.4035091 , -0.353941  ,\n",
      "        0.52811074,  0.5338562 , -0.09701867, -0.49369   ,  0.4283861 ,\n",
      "        0.02411905, -0.29138976,  0.87477326,  0.23499991,  0.17642944,\n",
      "        0.0646494 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#  extract features training_data\n",
    "train_input_vec = [[nlp(v).vector] for v in train['input']]  \n",
    "test_input_vec = [[nlp(v).vector] for v in test['input']]\n",
    "\n",
    "print(\"Length of One word Vector:\", len(train_input_vec[0][0]))\n",
    "print(\"Word Vector Example: \", train_input_vec[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore how the accuracy changes as we change the kernel_size. \n",
    "Create a model and try changing the kernel_size and padding.\n",
    "\n",
    "Kernel   Padding\n",
    "\n",
    "2           1\n",
    "\n",
    "3           1\n",
    "\n",
    "4           2\n",
    "\n",
    "5           2\n",
    "\n",
    "For each pair run the model five times and average the accuracy.  How does mean accuracy change?\n",
    "*Hint: you will have to change the linear dimentions to 1600, 1536, 1600, and 1536 correspondingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, nclasses:int, linear_dimension:int,kernel_size, padding, window_size: int = 16, embedding_dim: int = 16,\n",
    "    filter_multiplier: int = 64):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.simpleconv = nn.Conv2d(in_channels=1, out_channels=filter_multiplier, stride=CONV_STRIDE, kernel_size=kernel_size, padding=padding)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=CONV_STRIDE, padding=padding)\n",
    "        self.linear = nn.Linear(linear_dimension, out_features=nclasses)\n",
    "        self.softmax = nn.LogSoftmax(1)\n",
    "    \n",
    "    def forward(self, x, **kwargs): # shape: (batch, 16, 96)\n",
    "        # input x is already embedding vectors from spaCy\n",
    "        # set the channel dim to a new dimension that’s just 1\n",
    "        x = torch.transpose(x, 1, 2) # (batch, 1, 96)\n",
    "        x = torch.unsqueeze(x, 1) # (batch, 1, 96, 1)\n",
    "        x = self.simpleconv(x) # (batch, 64, 48, 1)\n",
    "        x = self.maxpool(x) # (batch, 64, 24, 1) \n",
    "        F.relu(x) # non-linear function to activate the neurons\n",
    "        x = x.flatten(start_dim=1) # (batch, 1536)\n",
    "        x = self.linear(x)\n",
    "        F.relu(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
    "                              SequentialSampler)\n",
    "\n",
    "def run_cnn_text(num_classes, batch_size, learning_rate, num_epochs, CONV_STRIDE, linear_dimention, kernel_size, padding):\n",
    "  model = SimpleCNN(num_classes, linear_dimention, kernel_size, padding) \n",
    "  # Create Dataloaders\n",
    "  train_inputs_tensor, test_inputs_tensor, train_label_tensor, test_label_tensor = tuple(torch.tensor(data) for data in[train_input_vec, test_input_vec, train['categorical_label'], test['categorical_label']])\n",
    "  # Create DataLoader for training data\n",
    "  train_data = TensorDataset(train_inputs_tensor, train_label_tensor)\n",
    "  train_sampler = RandomSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "  # Create DataLoader for testidation data\n",
    "  test_data = TensorDataset(test_inputs_tensor, test_label_tensor)\n",
    "  test_sampler = SequentialSampler(test_data)\n",
    "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.8)\n",
    "  lossfn = nn.NLLLoss()\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.\n",
    "    for i, batch in enumerate(train_dataloader): # create a new generator every epoch\n",
    "      input, label  = b_input_ids, b_labels = tuple(t for t in batch)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(input)\n",
    "      loss = lossfn(outputs, label)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "\n",
    "\n",
    "  model = model.eval()\n",
    "  num_correct = 0\n",
    "  num_examples = 0\n",
    "  # Check accuracy\n",
    "  for i, batch in enumerate(test_dataloader):\n",
    "    y, target = batch\n",
    "    output = model(y)\n",
    "    y_pred = []\n",
    "    for o in output:\n",
    "      y_pred.append(torch.max(o, -1).indices.item()) # Choose class with highest option\n",
    "    y_pred = torch.FloatTensor(y_pred)\n",
    "    correct = torch.eq(torch.round(y_pred).type(target.type()), target).view(-1)\n",
    "    num_correct += torch.sum(correct).item()\n",
    "    num_examples += correct.shape[0]\n",
    "  print(\"Accuracy: \",  num_correct / num_examples)\n",
    "  return(num_correct / num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.684\n",
      "Accuracy:  0.66\n",
      "Accuracy:  0.664\n",
      "Accuracy:  0.652\n",
      "Accuracy:  0.668\n",
      "Kernel = 2: 0.6656000000000001\n",
      "Accuracy:  0.662\n",
      "Accuracy:  0.658\n",
      "Accuracy:  0.674\n",
      "Accuracy:  0.654\n",
      "Accuracy:  0.654\n",
      "Kernel = 3: 0.6604\n",
      "Accuracy:  0.694\n",
      "Accuracy:  0.68\n",
      "Accuracy:  0.698\n",
      "Accuracy:  0.686\n",
      "Accuracy:  0.678\n",
      "Kernel = 4: 0.6872\n",
      "Accuracy:  0.702\n",
      "Accuracy:  0.698\n",
      "Accuracy:  0.694\n",
      "Accuracy:  0.702\n",
      "Accuracy:  0.672\n",
      "Kernel = 5: 0.6936\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "num_classes = 6 \n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "num_epochs = 50\n",
    "CONV_STRIDE = 2\n",
    "linear_dimension = 1600\n",
    "kernel_size = 2\n",
    "padding = 1\n",
    "\n",
    "test_accuracy = []\n",
    "for i in range(5):\n",
    "    test_accuracy.append(run_cnn_text(num_classes, batch_size, learning_rate, num_epochs, CONV_STRIDE, linear_dimension, kernel_size, padding))\n",
    "print(\"Kernel = 2:\", np.mean(test_accuracy))\n",
    "\n",
    "linear_dimension = 1536\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "test_accuracy = []\n",
    "for i in range(5):\n",
    "    test_accuracy.append(run_cnn_text(num_classes, batch_size, learning_rate, num_epochs, CONV_STRIDE, linear_dimension, kernel_size, padding))\n",
    "print(\"Kernel = 3:\", np.mean(test_accuracy))\n",
    "\n",
    "linear_dimension = 1600\n",
    "kernel_size = 4\n",
    "padding = 2\n",
    "test_accuracy = []\n",
    "for i in range(5):\n",
    "    test_accuracy.append(run_cnn_text(num_classes, batch_size, learning_rate, num_epochs, CONV_STRIDE, linear_dimension, kernel_size, padding))\n",
    "print(\"Kernel = 4:\", np.mean(test_accuracy))\n",
    "\n",
    "linear_dimension = 1536\n",
    "kernel_size = 5\n",
    "padding = 2\n",
    "test_accuracy = []\n",
    "for i in range(5):\n",
    "    test_accuracy.append(run_cnn_text(num_classes, batch_size, learning_rate, num_epochs, CONV_STRIDE, linear_dimension, kernel_size, padding))\n",
    "print(\"Kernel = 5:\", np.mean(test_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "694a2605e6c4cb7e5ecd12cf55a62475b886d9de17fe0247424478eb05601482"
  },
  "kernelspec": {
   "display_name": "Python 3.8.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
