{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> Week 2: Multilayer Perceptrons and Stochastic Gradient Descent </h1></center>\n",
    "<center> Jillian Fisher, Zaid Harchaoui </center>\n",
    "    <center> Data 598 (Winter 2023), University of Washington </center>\n",
    "    <center>  </center>\n",
    "\n",
    "This notebook is inspired by the [D2L book](https://d2l.ai/) and adapted from lecture material created by Krishna Pillutla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.nn.functional import cross_entropy\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: PyTorch Preliminaries\n",
    "PyTorch's automatic differentiation works by building a \n",
    "\"computation graph\" of all operations on tensors with `requires_grad=True`. \n",
    "It then computes the requested gradients by performing \n",
    "reverse-mode automatic differentiation, a.k.a. backpropagation. \n",
    "\n",
    "![Computational graph](https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png)\n",
    "\n",
    "There are number of caveats to keep in mind:\n",
    "\n",
    "- Automatic differentiation only works for floating point types (float32, float64). It does not work for integer types. (Think about why this is the case).\n",
    "- \"Building a computation graph\" takes up space. The memory usage increases proportional to the number of intermediate quantities stored.\n",
    "\n",
    "There are number of operations one might want to perform without building a computational graph. Examples include:\n",
    "\n",
    "- SGD updates\n",
    "- Accuracy computation (in general, logging utilies)\n",
    "\n",
    "PyTorch allows you to \"hide\" computations from autograd using\n",
    "`torch.no_grad()` (for example, see the accuracy computation functions below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0.1: Automatic Differentiation Practice\n",
    "\n",
    "Use PyTorch's automatic differentiation routines to compute the following derivatives/gradients and verify them on sample inputs. The solution to the first one is provided as an example.\n",
    "\n",
    "1. $f : \\mathbb{R}^3 \\to \\mathbb{R}$ defined by $f(x) = \\|x\\| = \\sqrt{x_1^2 + x_2^2 + x_3^2}$. Find $\\nabla_x f(x)$ for some $x\\neq 0$ (note that $f$ is not differentiable at $0$).\n",
    "1. $g: \\mathbb{R}^3 \\to \\mathbb{R}$ defined by $g(y) = \\frac{\\exp(y_1)}{\\exp(y_1) + \\exp(y_2) + \\exp(y_3)}$. Find $\\nabla_y g(y)$.\n",
    "1. $h: \\mathbb{R}^2 \\times \\mathbb{R}^3 \\to \\mathbb{R}$ with $h(a, b) = a^\\top M b$ and $M\\in \\mathbb{R}^{2 \\times 3}$ is the matrix of all ones (i.e., $M_{ij}=1$ for each $i, j$). Find $\\nabla_a h(a, b)$ and $\\nabla_b h(a, b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1820, -0.8567,  1.1006], requires_grad=True) tensor([ 0.1294, -0.6091,  0.7825]) tensor([ 0.1294, -0.6091,  0.7825], grad_fn=<DivBackward0>)\n",
      "tensor([ 0.1820, -0.8567,  1.1006], requires_grad=True) tensor([ 0.1294, -0.6091,  0.7825]) tensor([ 0.1294, -0.6091,  0.7825], grad_fn=<DivBackward0>)\n",
      "tensor([-1.0712,  0.1227, -0.5663], requires_grad=True) tensor([ 0.1397, -0.0930, -0.0467])\n",
      "tensor([ 0.3731, -0.8920], requires_grad=True) tensor([-1.5091,  0.3704,  1.4565], requires_grad=True) (tensor([0.3178, 0.3178]), tensor([-0.5189, -0.5189, -0.5189]))\n"
     ]
    }
   ],
   "source": [
    "## Solution to Part 1 in two ways:\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "true_gradient = x / torch.norm(x)  # computed analytically for verification\n",
    "\n",
    "# (a) Using the `.backward()` call\n",
    "output = torch.norm(x)\n",
    "output.backward()\n",
    "gradient = x.grad \n",
    "print(x, gradient, true_gradient)\n",
    "\n",
    "# (b) Using the `torch.autograd.grad` call\n",
    "output = torch.norm(x)\n",
    "# `torch.autograd.grad` returns a list. It has only one entry in this example\n",
    "gradient = torch.autograd.grad(outputs=output, inputs=[x])[0]\n",
    "print(x, gradient, true_gradient)\n",
    "\n",
    "## Solution to Part 2\n",
    "# <Your code here>\n",
    "y = torch.randn(3, requires_grad=True)\n",
    "output = torch.exp(y[0])/torch.sum(torch.exp(y))\n",
    "output.backward()\n",
    "gradient = y.grad\n",
    "print(y, gradient)\n",
    "\n",
    "## Solution to Part 3\n",
    "# <Your code here>\n",
    "M = torch.ones(2, 3, requires_grad=False)\n",
    "a = torch.randn(2, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "output = torch.dot(a, torch.matmul(M,b))\n",
    "gradient = torch.autograd.grad(outputs=output, inputs=[a,b])\n",
    "print(a,b,gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now generalize these examples to high dimensions.\n",
    "\n",
    "1. $f : \\mathbb{R}^n \\to \\mathbb{R}$ defined by $f(x) = \\|x\\| = \\sqrt{\\sum_{j=1}^n x_j^2}$. Find $\\nabla_x f(x)$ for some $x\\neq 0$ (note that $f$ is not differentiable at $0$).\n",
    "1. $g: \\mathbb{R}^3 \\to \\mathbb{R}$ defined by $g(y) = \\exp(y_1)/\\sum_{k=1}^n \\exp(y_k)$. Find $\\nabla_y g(y)$.\n",
    "1. $h: \\mathbb{R}^m \\times \\mathbb{R}^n \\to \\mathbb{R}$ with $h(a, b) = a^\\top M b$ and $M\\in \\mathbb{R}^{m \\times n}$ is the matrix of all ones (i.e., $M_{ij}=1$ for each $i\\in[m],j\\in[n]$). Find $\\nabla_a h(a, b)$ and $\\nabla_b h(a, b)$.\n",
    "\n",
    "For these examples, you may take $n = 100$ and $m = 50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution to Part 1 \n",
    "# <Your code here>\n",
    "\n",
    "## Solution to Part 2\n",
    "# <Your code here>\n",
    "\n",
    "## Solution to Part 3\n",
    "# <Your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: SGD for logistic regression versus the learning rate\n",
    "\n",
    "We will run SGD on the same fashion MNIST dataset as in the demo. Our goal here is to study the effect of the learning rate on the train/test loss/accuracy.\n",
    "\n",
    "Your task is to fill in the SGD update (look for `# <Your code here>`) and make the requested plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('./data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, idx in enumerate(np.random.choice(X_train.shape[0], 5)):\n",
    "    ax[i].imshow(X_train[idx], cmap='gray', vmin=0, vmax=255)\n",
    "    ax[i].set_title(f'Label = {y_train[idx]}', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Normalize them so the pixelwise mean is zero and standard deviation is 1\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)  # flatten into a (n, d) shape\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :]) / (std[None, :] + 1e-6)\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions to compute the objective and the accuracy\n",
    "def compute_objective(w, X, y, reg_param):\n",
    "    \"\"\" Compute the multinomial logistic loss. \n",
    "        Require w of shape (d,K), X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    score = torch.matmul(X, w)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return (\n",
    "        cross_entropy(input=score, target=y, reduction='mean') \n",
    "        + 0.5 * reg_param * torch.norm(w)**2\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(w, X, y):\n",
    "    \"\"\" Compute the classification accuracy\n",
    "        Require w of shape (d,K), X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    score = torch.matmul(X, w)\n",
    "    predictions = torch.argmax(score, axis=1)  # class with highest score is predicted\n",
    "    # <Your code here>: compute the accuracy from predictions and y\n",
    "    # Return the fraction of predictions that are correct\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logs(w, reg_param, verbose=False):\n",
    "    train_loss = compute_objective(w, X_train, y_train, reg_param)\n",
    "    test_loss = compute_objective(w, X_test, y_test, reg_param)\n",
    "    train_accuracy = compute_accuracy(w, X_train, y_train)\n",
    "    test_accuracy = compute_accuracy(w, X_test, y_test)\n",
    "    if verbose:\n",
    "        print(('Train Loss = {:.3f}, Train Accuracy = {:.3f}, ' + \n",
    "               'Test Loss = {:.3f}, Test Accuracy = {:.3f}').format(\n",
    "                train_loss.item(), train_accuracy.item(), \n",
    "                test_loss.item(), test_accuracy.item())\n",
    "    )\n",
    "    return (train_loss, train_accuracy, test_loss, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_one_pass(w, X, y, reg_param, learning_rate, verbose=False):\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "        idx = np.random.choice(X.shape[0])\n",
    "        # compute the objective. \n",
    "        # Note: This function requires X to be of shape (n,d). In this case, n=1    \n",
    "        objective = compute_objective(w, X[idx:idx+1], y[idx:idx+1], reg_param) \n",
    "        \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        gradient = torch.autograd.grad(outputs=objective, inputs=w)[0]\n",
    "        \n",
    "        # perform SGD update\n",
    "        with torch.no_grad():\n",
    "            # <Your code here>\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the divergent learning rate. \n",
    "\n",
    "Recall the definition of the divergent learning rate. We say ðœ‚â‹† is the divergent learning rate if SGD with a learning rate of 2ðœ‚â‹† diverges, but SGD with a learning rate of ðœ‚â‹† does not. \n",
    "\n",
    "Edit the code below to change the learning rate. \n",
    "\n",
    "*Hint 1*: Try different orders of magnitude. For instance, start with 1e-2. If it diverges, try 1e-6, if not, try 10.0. Once we establish a lower and upper bound on it, finding the divergent learning rate comes down to a binary search (on a logarithmic scale). \n",
    "\n",
    "*Hint 2*: A common strategy is to search in powers of 10 (as in hint 1), and then narrow the search down in powers of 2. For instance, if we end up with 1e-3 as an estimate of the divergent learning from hint 1, try out 2e-3 and 4e-3 as well in order to refine the estimate of the divergent learning rate.\n",
    "\n",
    "**Note**: the divergent learning rate is a heuristic which gives the right ballpark figure of the learning rate. \n",
    "A learning rate that satisfies the heuristic might still result in the loss diverging, in which case we would have redo the process with half that learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m reg_param \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m      4\u001b[0m w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m784\u001b[39m, n_class, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m _ \u001b[39m=\u001b[39m compute_logs(w, reg_param, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      8\u001b[0m w \u001b[39m=\u001b[39m sgd_one_pass(w, X_train, y_train, reg_param, learning_rate, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m _ \u001b[39m=\u001b[39m compute_logs(w, reg_param, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/data598/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mcompute_logs\u001b[0;34m(w, reg_param, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m test_accuracy \u001b[39m=\u001b[39m compute_accuracy(w, X_test, y_test)\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     30\u001b[0m     \u001b[39mprint\u001b[39m((\u001b[39m'\u001b[39m\u001b[39mTrain Loss = \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m, Train Accuracy = \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \n\u001b[1;32m     31\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mTest Loss = \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m, Test Accuracy = \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mformat(\n\u001b[0;32m---> 32\u001b[0m             train_loss\u001b[39m.\u001b[39mitem(), train_accuracy\u001b[39m.\u001b[39;49mitem(), \n\u001b[1;32m     33\u001b[0m             test_loss\u001b[39m.\u001b[39mitem(), test_accuracy\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m (train_loss, train_accuracy, test_loss, test_accuracy)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "learning_rate = 1.0\n",
    "reg_param = 0.0\n",
    "\n",
    "w = torch.zeros(784, n_class, requires_grad=True)\n",
    "_ = compute_logs(w, reg_param, verbose=True)\n",
    "\n",
    "\n",
    "w = sgd_one_pass(w, X_train, y_train, reg_param, learning_rate, verbose=True)\n",
    "_ = compute_logs(w, reg_param, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the following function which runs SGD for some number of passes through the data and tracks the test accuracy at the end of each pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: X_train, y_train, X_test, y_test are global variables\n",
    "def run_sgd(w, reg_param, learning_rate, num_passes=20, verbose=True):\n",
    "    logs = []\n",
    "\n",
    "    log_statistics = # <Your code here>; \n",
    "    #call `compute_logs` and track the train/test loss/accuracy\n",
    "    logs.append(log_statistics)\n",
    "    for j in range(num_passes):\n",
    "        # <Your code here>. Hint: call sgd_one_pass\n",
    "        log_statistics = # <Your code here>; \n",
    "        #call `compute_logs` and track the train/test loss/accuracy\n",
    "        logs.append(log_statistics)\n",
    "    return w, logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SGD for the learning rates from the list `[1e-2, 1e-3, 1e-4, 1e-5]` for 10 epochs. \n",
    "Plot 4 curves:\n",
    "\n",
    " - Train loss vs number of passes,\n",
    " - Train accuracy vs number of passes,\n",
    " - Test loss vs number of passes,\n",
    " - Test accuracy vs number of passes.\n",
    " \n",
    " Each curve must contain multiple lines, each corresponding to a different learning rate. \n",
    " You may use `reg_param=1e-3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# <Your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The effect of initialization and learning rate on MLPs\n",
    "We will repeat the same experiment with MLPs.\n",
    "\n",
    "Start by filling in the blank in the `sgd_one_pass` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions to compute the objective and the accuracy\n",
    "\n",
    "def mlp(X, ws, bs):\n",
    "    hidden = X # (n, d_0)\n",
    "    for w, b in zip(ws[:-1], bs[:-1]):\n",
    "        hidden = torch.matmul(hidden, w) + b[None, :]  # (n, d_{j-1}) * (d_{j-1}, d_j) = (n, d_j)\n",
    "        hidden = torch.nn.functional.relu(hidden)\n",
    "    return torch.matmul(hidden, ws[-1]) + bs[-1][None, :]\n",
    "\n",
    "def compute_objective_mlp(ws, bs, X, y, reg_param):\n",
    "    \"\"\" Compute the multinomial logistic loss. \n",
    "        ws is a list of tensors of consistent shapes,\n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    score = mlp(X, ws, bs)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return (\n",
    "        cross_entropy(input=score, target=y, reduction='mean') \n",
    "        + 0.5 * reg_param * sum([torch.norm(w)**2 for w in ws])\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy_mlp(ws, bs, X, y):\n",
    "    \"\"\" Compute the classification accuracy\n",
    "        ws is a list of tensors of consistent shapes \n",
    "        X of shape (n, d) and y of shape (n,)\n",
    "    \"\"\"\n",
    "    score = mlp(X, ws, bs)\n",
    "    predictions = torch.argmax(score, axis=1)  # class with highest score is predicted\n",
    "    return (predictions == y).sum() * 1.0 / y.shape[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logs_mlp(ws, bs, reg_param, verbose=False):\n",
    "    train_loss = compute_objective_mlp(ws, bs, X_train, y_train, reg_param)\n",
    "    test_loss = compute_objective_mlp(ws, bs, X_test, y_test, reg_param)\n",
    "    train_accuracy = compute_accuracy_mlp(ws, bs, X_train, y_train)\n",
    "    test_accuracy = compute_accuracy_mlp(ws, bs, X_test, y_test)\n",
    "    if verbose:\n",
    "        print(('Train Loss = {:.3f}, Train Accuracy = {:.3f}, ' + \n",
    "               'Test Loss = {:.3f}, Test Accuracy = {:.3f}').format(\n",
    "                train_loss.item(), train_accuracy.item(), \n",
    "                test_loss.item(), test_accuracy.item())\n",
    "    )\n",
    "    return (train_loss, train_accuracy, test_loss, test_accuracy)\n",
    "\n",
    "def sgd_one_pass_mlp(ws, bs, X, y, reg_param, learning_rate, verbose=False):\n",
    "    # Each pass through the data is called an `epoch`.\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "        idx = np.random.choice(X.shape[0])\n",
    "        # compute the objective. \n",
    "        # Note: This function requires X to be of shape (n,d). In this case, n=1 \n",
    "        objective = compute_objective_mlp(ws, bs, X[idx:idx+1], y[idx:idx+1], reg_param) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        all_parameters = [*ws, *bs]\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=all_parameters)\n",
    "        \n",
    "        # perform SGD update. IMPORTANT: Make the update inplace!\n",
    "        with torch.no_grad():\n",
    "            # TODO: <Your code here>\n",
    "    return ws, bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the divergent learning rate. Edit the code below to change the learning rate and see what you observe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "hidden_size = 1024\n",
    "ws = [1e-6 * torch.randn(784, hidden_size, requires_grad=True),\n",
    "      1e-6 * torch.randn(hidden_size, n_class, requires_grad=True)]\n",
    "bs = [torch.zeros(hidden_size, requires_grad=True),\n",
    "      torch.zeros(n_class, requires_grad=True)]\n",
    "\n",
    "_ = compute_logs_mlp(ws, bs, reg_param, verbose=True)\n",
    "\n",
    "\n",
    "ws, bs = sgd_one_pass_mlp(ws, bs, X_train, y_train, reg_param, learning_rate, verbose=True)\n",
    "_ = compute_logs_mlp(ws, bs, reg_param, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will play with the initialization.\n",
    "\n",
    "- Try initializing with every W_j, b_j be 0 everywhere. What happens?\n",
    "- Try W_j, b_j be 1 everywhere. What happens?\n",
    "\n",
    "For these two cases, compute the gradient $\\nabla f(w)$. What do you observe? Does this explain why these initializations are unsuitable for stochastic gradient optimization?\n",
    "\n",
    "- Try increasing the variance from 0 to 1e-20, 1e-10, 1e-6, 1e-3 and 1e-1. What do you observe?\n",
    "- Try out the so-called Glorot-Bengio initialization scheme, where $W_j \\in \\mathbb{R}^{d_{j-1}\\times d_j}$ is taken to be be normally distributed with variance $1/d_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2.5e-3\n",
    "\n",
    "# TODO: change the initialization\n",
    "ws = [1e-6 * torch.randn(784, hidden_size, requires_grad=True),\n",
    "      1e-6 * torch.randn(hidden_size, n_class, requires_grad=True)]\n",
    "bs = [torch.zeros(hidden_size, requires_grad=True),\n",
    "      torch.zeros(n_class, requires_grad=True)]\n",
    "_ = compute_logs(ws, bs, reg_param, verbose=True)\n",
    "\n",
    "\n",
    "ws = sgd_one_pass(ws, bs, X_train, y_train, reg_param, learning_rate, verbose=True)\n",
    "_ = compute_logs(ws, bs, reg_param, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a function which makes a certain number of passes of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sgd_mlp(ws, bs, reg_param, learning_rate, num_passes=10, verbose=True):\n",
    "    logs = [compute_logs_mlp(ws, bs, reg_param, verbose=verbose)]\n",
    "    # We run training for multiple epochs. Each epoch is a pass through the data.\n",
    "    for j in range(num_passes):\n",
    "        ws, bs = sgd_one_pass_mlp(ws, bs, X_train, y_train, \n",
    "                          reg_param, learning_rate, verbose=False)\n",
    "        logs.append(compute_logs_mlp(ws, bs, reg_param, verbose=verbose))\n",
    "    return ws, bs, np.asarray(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SGD for the learning rates from the list `[1e-2, 1e-3, 1e-4, 1e-5]` for 10 epochs. \n",
    "Plot 4 curves:\n",
    "\n",
    " - Train loss vs number of passes,\n",
    " - Train accuracy vs number of passes,\n",
    " - Test loss vs number of passes,\n",
    " - Test accuracy vs number of passes.\n",
    " \n",
    " Each curve must contain multiple lines, each corresponding to a different learning rate. \n",
    " You may use `reg_param=0`. \n",
    " \n",
    " Note: This takes several minutes to run. It took me 15 min for everything to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: <Your code here>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "86858d5f84902b050d54a6bd9c0fb878f86a1dbd7a1ae5d42a558ee921f88707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
