{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Lecture 6: Embeddings and Model Selection </h1> </center>\n",
    "<center> Jillian Fisher, Zaid Harchaoui </center>\n",
    "    <center> Data 598 (Winter 2023), University of Washington </center>\n",
    "\n",
    "We will discuss two topics this lecture:\n",
    "- Embeddings for natural language\n",
    "- Model Selection with statistical tests\n",
    "\n",
    "\n",
    "The first part of this notebook has been adapted from the [D2L book]( http://d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html),  and adapted from lecture material created by Krishna Pillutla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Embeddings for Natural Language\n",
    "\n",
    "The field of **natural language processing (NLP)** is concerned with the interaction between computers and natural (human) language. This involves \"understanding\" the contents of documents, including the contextual nuances of the language within them. \n",
    "\n",
    "**Embeddings**:\n",
    "The use of machine learning for NLP, both in the classical settings as well as the modern deep learning era, have relied on *embedding* words in vector spaces.\n",
    "Words are made of characters, which are combinatorial in nature with no \"neighborhood\" structure which one expects of vectors in, say, a Euclidean space. \n",
    "The magic of embeddings is that they are able to capture some \"neighborhood\" structure in words, e.g., the embedding of semantically similar words are closer together than of words which have nothing in common. \n",
    "\n",
    "![](https://miro.medium.com/max/2400/1*OEmWDt4eztOcm5pr2QbxfA.png)\n",
    "Image credits: https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8\n",
    "\n",
    "**Note**: Sometimes, we will work at the level of subword units, rather than words. Mathematically, the same treatment holds irrespective of how we *tokenize* the text. We will refer to these units as *tokens*.\n",
    "\n",
    "\n",
    "**Types of embeddings**:\n",
    "\n",
    "- Global embeddings: word2vec, GloVe\n",
    "- Contextual embeddings: ELMo, BERT, ...\n",
    "\n",
    "In this lab, we will play with the GloVe embeddings, which are global embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the GloVe embedding ~66M compressed + 164M uncompressed\n",
    "import os\n",
    "if 'glove.6B.50d' not in os.listdir():\n",
    "    !wget http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip\n",
    "    !unzip glove.6B.50d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We index each word in our dictionary using integers. \n",
    "We store the following mappings:\n",
    "- word $\\to$ index\n",
    "- index $\\to$ word\n",
    "- index $\\to$ embedding of the corresponding word\n",
    "\n",
    "Words not in our dictionary are denoted using the `<unk>` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVeWordEmbedding:\n",
    "    def __init__(self):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding()\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        with open('glove.6B.50d/vec.txt') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, np.asarray(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        # \"tokens\" is a list of words\n",
    "        # use as object[tokens]\n",
    "        # map token -> index -> vector\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[np.asarray(indices)]\n",
    "        return vecs\n",
    "    \n",
    "    def __call__(self, tokens):\n",
    "        # Use as object(tokens)\n",
    "        return self.__getitem__(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = GloVeWordEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by noting that the embedding of a word does not depend on its context. Recall that GloVe is a global (non-contextual) embedding in contrast to contextual embeddings such as BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 50)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the embeddings of words:\n",
    "sentence1 = 'I love data science'\n",
    "embeddings1 = glove_embedding[sentence1.split()]  # using the __getitem__ method\n",
    "# alternatively: \n",
    "# embeddings1 = glove_embedding(sentence1.split())  # using the __call__ method\n",
    "print(embeddings1.shape)  # (number of words, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 50)\n"
     ]
    }
   ],
   "source": [
    "sentence2 = 'As a kid, I always wanted to study mathematics and science'\n",
    "embeddings2 = glove_embedding[sentence2.split()]\n",
    "print(embeddings2.shape)  # (number of words, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the embeddings of both the time the word \"science\" appears\n",
    "e1 = embeddings1[-1]\n",
    "e2 = embeddings2[-1]\n",
    "print(np.linalg.norm(e1-e2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look at the cosine similarity between word embeddings. \n",
    "Recall that the cosine similarity between two vectors $u, v \\in \\mathbb{R}^d$ is defined as \n",
    "$$\n",
    "    S_{\\cos}(u, v) = \\frac{\\langle u, v\\rangle}{\\|u\\|_2  \\, \\|v\\|_2} .\n",
    "$$\n",
    "\n",
    "For any pair of vectors, the cosine similarity is always \n",
    "between $-1$ and $1$. (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the k nearest neighbors \n",
    "# as per cosine similarity\n",
    "def k_nearest_neighbors(population, query, k):\n",
    "    # population is a matrix of size (n, dim)\n",
    "    # query is a matrix of shape (1, dim)\n",
    "    # k is an integer\n",
    "    # return topk indices and topk values\n",
    "    cos = np.dot(population, query.reshape(-1,)) / (\n",
    "        np.sqrt(np.sum(population * population, axis=1) + 1e-9) *\n",
    "        np.linalg.norm(query.reshape(-1))\n",
    "    )\n",
    "    topk_idx = np.argpartition(cos, -k)[-k:] # unsorted\n",
    "    topk_idx = topk_idx[np.argsort(cos[topk_idx])][::-1]  # sorted\n",
    "    topk_val = cos[topk_idx]\n",
    "    return topk_idx, topk_val\n",
    "\n",
    "# Hint for topk in numpy: https://stackoverflow.com/a/23734295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 microwave 0.9999999999839023\n",
      "1 analog 0.7300107691175304\n",
      "2 microwaves 0.7264979322621885\n",
      "3 oven 0.7115686481570183\n",
      "4 refrigerator 0.7039825402692076\n",
      "5 ovens 0.6948281242683833\n"
     ]
    }
   ],
   "source": [
    "query = glove_embedding(['microwave'])\n",
    "topk_idxs, topk_vals = k_nearest_neighbors(glove_embedding.idx_to_vec, query, 6)\n",
    "topk_words = [glove_embedding.idx_to_token[i] for i in topk_idxs]\n",
    "\n",
    "for i, (w, val) in enumerate(zip(topk_words, topk_vals)):\n",
    "    print(i, w, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 serendipity 0.9999999999485274\n",
      "1 profundity 0.7222786942557122\n",
      "2 happenstance 0.7016944428733708\n",
      "3 strangeness 0.6975966805924638\n",
      "4 weirdness 0.6901399894427984\n",
      "5 hokum 0.6882356254537078\n"
     ]
    }
   ],
   "source": [
    "query = glove_embedding(['serendipity'])\n",
    "topk_idxs, topk_vals = k_nearest_neighbors(glove_embedding.idx_to_vec, query, 6)\n",
    "topk_words = [glove_embedding.idx_to_token[i] for i in topk_idxs]\n",
    "\n",
    "for i, (w, val) in enumerate(zip(topk_words, topk_vals)):\n",
    "    print(i, w, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies with word embeddings\n",
    "\n",
    "In addition to seeking synonyms, we can also use the pretrained word vector to seek the analogies between words. For example, “man”:“woman”::“son”:“daughter” is an example of analogy, “man” is to “woman” as “son” is to “daughter”. \n",
    "\n",
    "The problem of seeking analogies can be defined as follows: for four words in the analogical relationship $a:b::c:d$, given the first three words, a, b and c, we want to find d. \n",
    "\n",
    "Assume the word vector for the word w is $\\text{vec}(w)$. To solve the analogy problem, we need to find the word vector that is most similar to the result vector of $\\text{vec}(c)+\\text{vec}(b)−\\text{vec}(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c):\n",
    "    # Implement the analogy from above\n",
    "    vecs = glove_embedding[[token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = k_nearest_neighbors(glove_embedding.idx_to_vec, x, 1)\n",
    "    return glove_embedding.idx_to_token[int(topk[0])]  # Remove unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daughter'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_analogy('man', 'woman', 'son')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# capital-country\n",
    "get_analogy('london', 'england', 'paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'looked'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tense\n",
    "get_analogy('dance', 'danced', 'look')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Selection with Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run a test to compare two different models. This is called the McNemar's test. \n",
    "\n",
    "Let $h_1$ and $h_2$ be two different classification algorithms. \n",
    "The hypotheses we're testing are:\n",
    "$$\n",
    "H_0 : \\quad \\text{acc}(h_1) = \\text{acc}(h_2) \\\\\n",
    "H_1 : \\quad \\text{acc}(h_1) \\ne \\text{acc}(h_2) ,\n",
    "$$\n",
    "where acc $(h)$ is the accuracy of the classifier $h$.\n",
    "\n",
    "\n",
    "To distinguish between the two, we compute the following numbers:\n",
    "- $N_{01}$: the number of validation examples misclassified by $h_1$ but correctly classified by $h_2$\n",
    "- $N_{10}$: the number of validation examples correctly classified by $h_1$ but misclassified by $h_2$. \n",
    "\n",
    "The test statistic is \n",
    "$$\n",
    "    T = \\frac{(|N_{01} - N_{10}| - 1)^2}{N_{10} + N_{01}}.\n",
    "$$\n",
    "Its asymptotic distribution under the null is $\\chi^2$-distribution with $1$ degree of freedom. We reject the test null hypothesis if \n",
    "$$T > \\chi^2_{1, \\alpha}$$\n",
    "the $(1-\\alpha)$-quantile of the $\\chi^2_1$ distribution.\n",
    "\n",
    "**The exercise**:\n",
    "Run this hypothesis test to compare the MLP from week 1 and the ConvNet from week 2. Use a significance $\\alpha=0.01$. Train each network with SGD for $30$ passes with an appropriate learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torch.nn.functional import cross_entropy\n",
    "import time\n",
    "import scipy.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('./data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "# idxs_test = torch.from_numpy(\n",
    "#     np.random.choice(X_test.shape[0], replace=False, size=X_test.shape[0]//10))\n",
    "# X_test, y_test = X_test[idxs_test], y_test[idxs_test]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Normalize them so the pixelwise mean is zero and standard deviation is 1\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :]) / (std[None, :] + 1e-6)\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class MLP(torch.nn.Module): \n",
    "    def __init__(self, hidden_width=32):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(784, hidden_width)\n",
    "        self.linear2 = torch.nn.Linear(32, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_ensemble_1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.conv_ensemble_2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.fc = torch.nn.Linear(7*7*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        out = self.conv_ensemble_1(x)\n",
    "        out = self.conv_ensemble_2(out)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "# Some utility functions to compute the objective and the accuracy\n",
    "def compute_objective(model, X, y):\n",
    "    score = model(X)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return cross_entropy(input=score, target=y, reduction='mean') \n",
    "\n",
    "def sgd_one_pass(model, X, y, learning_rate, verbose=False):\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "        idx = np.random.choice(X.shape[0])\n",
    "        # compute the objective. \n",
    "        # Note: This function requires X to be of shape (n,d). In this case, n=1 \n",
    "        objective = compute_objective(model, X[idx:idx+1], y[idx:idx+1]) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=model.parameters())\n",
    "        \n",
    "        # perform SGD update. IMPORTANT: Make the update inplace!\n",
    "        for (w, g) in zip(model.parameters(), gradients):\n",
    "            w.data -= learning_rate * g.data\n",
    "      \n",
    "    \n",
    "from tqdm.auto import trange # range + progress bar\n",
    "def sgd_n_passes(model, X_train, y_train, X_val, y_val, n_passes, learning_rate):\n",
    "    for i in trange(n_passes):\n",
    "        sgd_one_pass(model, X_train, y_train, learning_rate)\n",
    "    return compute_prediction_performance(model, X_val, y_val)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_prediction_performance(model, X, y):\n",
    "    # return a boolean vector of the same length as y\n",
    "    # each entry is True if correctly predicted, else False\n",
    "    score = model(X)\n",
    "    predictions = torch.argmax(score, axis=1)  # class with highest score is predicted\n",
    "    return (predictions == y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7675447a1f1f46659fb3472c6729f460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = MLP()\n",
    "performance1 = sgd_n_passes(\n",
    "    model1, X_train, y_train, X_test, y_test, n_passes=30, learning_rate=2e-3\n",
    ")\n",
    "# boolean vector of length n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace669b833bc4fdd8d12b038734541d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = ConvNet()\n",
    "performance2 = sgd_n_passes(\n",
    "    model2, X_train, y_train, X_test, y_test, n_passes=30, learning_rate=2.5e-3\n",
    ")\n",
    "# boolean vector of length n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of MLP: 0.8324\n",
      "accuracy of ConvNet: 0.8698\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of MLP:', performance1.sum().item()/y_test.shape[0])\n",
    "print('accuracy of ConvNet:', performance2.sum().item()/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test whether this difference is statistically significant or not. From a \n",
    "first glance, it does appear to be statistically significant as the gap is quite large. \n",
    "\n",
    "Compute $N_{01}$ and $N_{10}$ from the output of SGD above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N01 = (~performance1 & performance2).sum().item()  # MLP is wrong but ConvNet is correct\n",
    "N10 = (performance1 & ~performance2).sum().item()  # MLP is correct but ConvNet is wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the test statistic, the threshold, which is the $(1-\\alpha)$ quantile of the $\\chi^2_1$ distribution and read off the conclusion of the test. Recall that we have $\\alpha=0.01$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: 128.58502772643254, threshold: 6.6348966010212145\n",
      "Null rejected\n"
     ]
    }
   ],
   "source": [
    "T = (abs(N01 - N10) - 1)**2 / (N01 + N10)  # statistic\n",
    "threshold = scipy.stats.chi2(df=1).ppf(0.99)\n",
    "\n",
    "print(f'Test statistic: {T}, threshold: {threshold}')\n",
    "\n",
    "if T > threshold:\n",
    "    print('Null rejected')\n",
    "else:\n",
    "    print('Failed to reject the null')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d660cccac8868f582cce1165226b101da62e876645a12a7619ce413de84bef5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
